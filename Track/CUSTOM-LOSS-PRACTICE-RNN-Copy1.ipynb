{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 12 16:23:53 2017       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 47%   72C    P2   128W / 180W |   2384MiB /  8114MiB |     98%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 1080    Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8     9W / 180W |     10MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 1080    Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8     9W / 180W |     10MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 1080    Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 27%   28C    P8     9W / 180W |    515MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 1080    Off  | 00000000:0B:00.0 Off |                  N/A |\n",
      "| 27%   29C    P8     9W / 180W |     10MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 1080    Off  | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 46%   69C    P2   109W / 180W |   7972MiB /  8114MiB |     95%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 1080    Off  | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8     9W / 180W |     10MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 1080    Off  | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 27%   30C    P8    10W / 180W |   8030MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 (0000:0D:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "################################################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, Flatten, TimeDistributed, Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from typing import Tuple, Callable, List, Optional, Sequence, Generator, Any\n",
    "from tracker import visuals, extractor, utils, metrics\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "np.set_printoptions(edgeitems=20)\n",
    "Tensor = theano.tensor.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Input Matrix ===\n",
      "(1024, 6, 3)\n",
      "[[[4 8 2]\n",
      "  [5 3 6]\n",
      "  [2 1 5]\n",
      "  [0 6 8]\n",
      "  [1 8 0]\n",
      "  [1 5 7]]\n",
      "\n",
      " [[6 3 8]\n",
      "  [3 5 8]\n",
      "  [6 2 5]\n",
      "  [2 5 1]\n",
      "  [7 1 5]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 3 8]\n",
      "  [8 8 8]\n",
      "  [8 7 3]\n",
      "  [6 2 3]\n",
      "  [4 6 0]\n",
      "  [4 7 2]]\n",
      "\n",
      " [[4 8 1]\n",
      "  [1 7 1]\n",
      "  [1 8 7]\n",
      "  [1 2 1]\n",
      "  [7 7 4]\n",
      "  [5 3 5]]\n",
      "\n",
      " [[5 6 6]\n",
      "  [8 0 0]\n",
      "  [2 7 7]\n",
      "  [6 1 3]\n",
      "  [6 5 8]\n",
      "  [1 2 6]]\n",
      "\n",
      " [[5 5 5]\n",
      "  [0 1 7]\n",
      "  [5 0 4]\n",
      "  [5 3 7]\n",
      "  [4 8 6]\n",
      "  [3 5 5]]\n",
      "\n",
      " [[2 6 7]\n",
      "  [0 8 6]\n",
      "  [5 2 0]\n",
      "  [4 3 8]\n",
      "  [5 7 2]\n",
      "  [3 6 6]]\n",
      "\n",
      " [[2 2 7]\n",
      "  [7 2 8]\n",
      "  [0 0 8]\n",
      "  [1 1 8]\n",
      "  [0 8 8]\n",
      "  [0 3 3]]\n",
      "\n",
      " [[2 0 6]\n",
      "  [3 6 4]\n",
      "  [2 5 0]\n",
      "  [5 0 7]\n",
      "  [5 5 4]\n",
      "  [0 8 6]]\n",
      "\n",
      " [[0 4 2]\n",
      "  [5 2 8]\n",
      "  [8 7 1]\n",
      "  [1 8 2]\n",
      "  [0 8 2]\n",
      "  [0 5 4]]\n",
      "\n",
      " [[8 6 4]\n",
      "  [7 2 1]\n",
      "  [1 8 0]\n",
      "  [2 1 3]\n",
      "  [0 6 3]\n",
      "  [7 7 2]]\n",
      "\n",
      " [[4 1 4]\n",
      "  [4 3 6]\n",
      "  [2 8 7]\n",
      "  [7 4 2]\n",
      "  [0 3 0]\n",
      "  [4 1 2]]\n",
      "\n",
      " [[5 6 2]\n",
      "  [7 6 5]\n",
      "  [5 8 8]\n",
      "  [7 6 6]\n",
      "  [8 3 7]\n",
      "  [2 2 6]]\n",
      "\n",
      " [[7 8 4]\n",
      "  [7 2 2]\n",
      "  [2 1 0]\n",
      "  [1 7 1]\n",
      "  [6 5 0]\n",
      "  [2 8 8]]\n",
      "\n",
      " [[6 4 8]\n",
      "  [3 5 8]\n",
      "  [8 2 2]\n",
      "  [4 6 7]\n",
      "  [4 2 5]\n",
      "  [3 0 3]]\n",
      "\n",
      " [[8 5 3]\n",
      "  [7 6 6]\n",
      "  [7 5 4]\n",
      "  [5 7 8]\n",
      "  [1 1 1]\n",
      "  [4 3 6]]\n",
      "\n",
      " [[0 1 0]\n",
      "  [7 8 7]\n",
      "  [8 3 4]\n",
      "  [4 6 7]\n",
      "  [5 4 8]\n",
      "  [1 0 5]]\n",
      "\n",
      " [[4 6 3]\n",
      "  [3 2 0]\n",
      "  [8 1 6]\n",
      "  [7 0 5]\n",
      "  [5 0 4]\n",
      "  [2 3 6]]\n",
      "\n",
      " [[5 6 6]\n",
      "  [4 2 6]\n",
      "  [0 4 0]\n",
      "  [8 5 3]\n",
      "  [2 3 3]\n",
      "  [5 5 8]]\n",
      "\n",
      " [[0 2 6]\n",
      "  [7 7 7]\n",
      "  [7 5 8]\n",
      "  [8 4 5]\n",
      "  [0 2 5]\n",
      "  [3 0 8]]\n",
      "\n",
      " ..., \n",
      " [[8 2 1]\n",
      "  [4 1 7]\n",
      "  [1 0 0]\n",
      "  [6 5 1]\n",
      "  [5 4 2]\n",
      "  [0 3 7]]\n",
      "\n",
      " [[1 5 5]\n",
      "  [2 6 4]\n",
      "  [0 2 3]\n",
      "  [5 7 8]\n",
      "  [4 4 1]\n",
      "  [3 1 8]]\n",
      "\n",
      " [[5 7 5]\n",
      "  [0 6 8]\n",
      "  [3 2 5]\n",
      "  [3 6 7]\n",
      "  [5 0 3]\n",
      "  [5 2 3]]\n",
      "\n",
      " [[5 0 1]\n",
      "  [4 8 5]\n",
      "  [7 6 2]\n",
      "  [5 0 8]\n",
      "  [5 5 1]\n",
      "  [6 1 7]]\n",
      "\n",
      " [[8 3 8]\n",
      "  [3 7 3]\n",
      "  [0 5 6]\n",
      "  [5 5 0]\n",
      "  [6 1 5]\n",
      "  [8 4 3]]\n",
      "\n",
      " [[0 2 6]\n",
      "  [8 5 5]\n",
      "  [6 6 0]\n",
      "  [2 4 0]\n",
      "  [2 5 7]\n",
      "  [2 0 1]]\n",
      "\n",
      " [[8 4 5]\n",
      "  [1 8 1]\n",
      "  [5 0 7]\n",
      "  [8 5 2]\n",
      "  [3 8 4]\n",
      "  [7 2 7]]\n",
      "\n",
      " [[2 6 7]\n",
      "  [0 0 3]\n",
      "  [8 3 2]\n",
      "  [6 7 0]\n",
      "  [8 4 7]\n",
      "  [0 8 4]]\n",
      "\n",
      " [[0 3 5]\n",
      "  [4 5 8]\n",
      "  [8 8 2]\n",
      "  [3 2 4]\n",
      "  [1 5 8]\n",
      "  [8 8 2]]\n",
      "\n",
      " [[7 3 6]\n",
      "  [5 3 6]\n",
      "  [4 0 7]\n",
      "  [7 0 6]\n",
      "  [1 8 3]\n",
      "  [4 0 6]]\n",
      "\n",
      " [[4 8 6]\n",
      "  [0 6 5]\n",
      "  [2 2 2]\n",
      "  [6 5 5]\n",
      "  [1 1 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " [[4 3 1]\n",
      "  [2 8 8]\n",
      "  [1 5 5]\n",
      "  [0 5 4]\n",
      "  [6 2 5]\n",
      "  [0 7 2]]\n",
      "\n",
      " [[1 0 4]\n",
      "  [1 0 6]\n",
      "  [2 5 5]\n",
      "  [8 7 5]\n",
      "  [4 1 4]\n",
      "  [3 7 0]]\n",
      "\n",
      " [[5 5 8]\n",
      "  [1 0 0]\n",
      "  [3 6 2]\n",
      "  [3 6 3]\n",
      "  [8 3 4]\n",
      "  [7 0 5]]\n",
      "\n",
      " [[7 6 3]\n",
      "  [3 3 7]\n",
      "  [5 4 7]\n",
      "  [8 7 7]\n",
      "  [5 2 5]\n",
      "  [1 2 3]]\n",
      "\n",
      " [[7 8 4]\n",
      "  [2 8 8]\n",
      "  [5 1 7]\n",
      "  [4 0 5]\n",
      "  [3 8 6]\n",
      "  [6 1 7]]\n",
      "\n",
      " [[8 0 5]\n",
      "  [3 4 1]\n",
      "  [6 4 0]\n",
      "  [4 7 6]\n",
      "  [7 4 8]\n",
      "  [7 0 0]]\n",
      "\n",
      " [[4 0 8]\n",
      "  [8 1 4]\n",
      "  [5 3 2]\n",
      "  [7 8 6]\n",
      "  [3 3 6]\n",
      "  [7 6 6]]\n",
      "\n",
      " [[3 3 2]\n",
      "  [7 0 6]\n",
      "  [7 0 5]\n",
      "  [4 8 5]\n",
      "  [3 2 8]\n",
      "  [7 5 4]]\n",
      "\n",
      " [[7 2 1]\n",
      "  [3 8 3]\n",
      "  [4 7 3]\n",
      "  [5 5 5]\n",
      "  [6 1 5]\n",
      "  [6 4 6]]]\n",
      "\n",
      "=== Output Matrix ===\n",
      "(1024, 6, 8)\n",
      "[[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " ..., \n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "def get_data(\n",
    "        num_vectors : int,\n",
    "        vector_size : int = 3,\n",
    "        min_value   : int = 0,\n",
    "        max_value   : int = 9,\n",
    "        ) -> np.ndarray:\n",
    "    return np.random.randint(min_value, max_value, (num_vectors, vector_size))\n",
    "\n",
    "def get_output(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a 1-hot categorical matrix from *data*.\n",
    "    *data* is a 2D array. Create output array by:\n",
    "    For each cell in *data*, transform to 1 if cell is even. Else, 0.\n",
    "    For each row, concatenate all the 1s and 0s into a single binary number.\n",
    "    The binary number corresponds to that row's category.\n",
    "    Example: If row = (2, 4, 80), then category = 000 = 0.\n",
    "    Example: If row = (1, 30, 9), then category = 101 = 5.\n",
    "    Example: If row = (3, 9, 12, 7), then category = 1101 = 13.\n",
    "    Example: If row = (2, 1), then category = 01 = 1.\n",
    "    \"\"\"\n",
    "    weights = np.array([2**i for i in range(data.shape[1])])[::-1]\n",
    "    output = np.zeros((len(data), 2**len(weights)))\n",
    "    indices = (data % 2) @ weights\n",
    "    output[np.arange(len(data)), indices] = 1\n",
    "    return output\n",
    "\n",
    "np.random.seed(1010)\n",
    "data   = np.array([get_data(num_vectors=6) for _ in range(1024)])\n",
    "output = np.array([get_output(matrix) for matrix in data])\n",
    "val_data   = np.array([get_data(num_vectors=6) for _ in range(512)])\n",
    "val_output = np.array([get_output(matrix) for matrix in val_data])\n",
    "print(\"=== Input Matrix ===\\n{0}\\n{1}\".format(data.shape, data))\n",
    "print(\"\\n=== Output Matrix ===\\n{0}\\n{1}\".format(output.shape, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss is purely the built-in categorical cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "input_layer  = Input(name=\"Input\", shape=data.shape[1:])\n",
    "model_layer  = Dropout(name=\"Dropout 1\", rate=1/2)(input_layer)\n",
    "model_layer  = Bidirectional(GRU(name=\"GRU 1\", units=512, return_sequences=True,\n",
    "                    recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "model_layer  = keras.layers.BatchNormalization()(model_layer)\n",
    "model_layer  = Dropout(name=\"Dropout 2\", rate=1/2)(model_layer)\n",
    "model_layer  = Bidirectional(GRU(name=\"GRU 2\", units=512, return_sequences=True,\n",
    "                   recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "model_layer  = Dropout(name=\"Dropout 3\", rate=1/2)(model_layer)\n",
    "model_layer  = Bidirectional(GRU(name=\"GRU 3\", units=512, return_sequences=True,\n",
    "                   recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "output_layer = Dense(name=\"Output\", units=output.shape[2], activation=\"softmax\")(model_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "################################################################################\n",
    "np.random.seed(1010)\n",
    "histories = model.fit(data, output, epochs=64, batch_size=1, verbose=2,\n",
    "                      validation_data=(val_data, val_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "plt.plot(histories.history['loss'])\n",
    "plt.plot(histories.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "plt.plot(histories.history['acc'])\n",
    "plt.plot(histories.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "test_input  = val_data\n",
    "test_output = val_output\n",
    "predictions = model.predict(test_input).round(2)\n",
    "print(\"\\nAvg Difference: {}\".format(np.abs(test_output - predictions).mean()))\n",
    "print(\"=== Input ===\\n{}\".format(test_input))\n",
    "print(\"\\n=== True Output ===\\n{}\".format(test_output))\n",
    "print(\"\\n=== Prediction ===\\n{}\".format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss is custom and input tensor is introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Input ===\n",
      "[[[4 8 2]\n",
      "  [5 3 6]\n",
      "  [2 1 5]\n",
      "  [0 6 8]\n",
      "  [1 8 0]\n",
      "  [1 5 7]]\n",
      "\n",
      " [[6 3 8]\n",
      "  [3 5 8]\n",
      "  [6 2 5]\n",
      "  [2 5 1]\n",
      "  [7 1 5]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 3 8]\n",
      "  [8 8 8]\n",
      "  [8 7 3]\n",
      "  [6 2 3]\n",
      "  [4 6 0]\n",
      "  [4 7 2]]\n",
      "\n",
      " [[4 8 1]\n",
      "  [1 7 1]\n",
      "  [1 8 7]\n",
      "  [1 2 1]\n",
      "  [7 7 4]\n",
      "  [5 3 5]]\n",
      "\n",
      " [[5 6 6]\n",
      "  [8 0 0]\n",
      "  [2 7 7]\n",
      "  [6 1 3]\n",
      "  [6 5 8]\n",
      "  [1 2 6]]]\n",
      "\n",
      "=== Prediction ===\n",
      "[[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]]\n",
      "\n",
      "=== Losses ===\n",
      "[ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "class InputTensorLossFunction:\n",
    "    def __init__(self,\n",
    "            input_shape  : Tuple[int, int],\n",
    "            output_shape : Tuple[int, int],\n",
    "            input_tensor : Tensor,\n",
    "            ) -> None:\n",
    "        self.output_shape = output_shape\n",
    "        self.input_shape  = input_shape\n",
    "        self.input_tensor = input_tensor\n",
    "    \n",
    "    def exponents_of_2_tensor(self, shape: Tuple[int, int]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Return matrix with shape equal to *shape* of the form:\n",
    "        [2, 4, 8, ..., 2**n]\n",
    "        [2, 4, 8, ..., 2**n]\n",
    "        ...\n",
    "        [2, 4, 8, ..., 2**n]\n",
    "        \"\"\"\n",
    "        m = [[2**i for i in range(shape[1])][::-1] for _ in range(shape[0])]\n",
    "        return T.as_tensor(m)\n",
    "    \n",
    "    def make_meshgrid(self, shape: Tuple[int, int]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Create a meshgrid with shape equal to *shape*.\n",
    "        Example for shape (3, 5):\n",
    "            [[0, 1, 2, 3, 4],\n",
    "             [0, 1, 2, 3, 4],\n",
    "             [0, 1, 2, 3, 4]]\n",
    "        \"\"\"\n",
    "        return (T.mgrid[0:shape[0], 0:shape[1]][1])\n",
    "    \n",
    "    def penalize_noncommitment(self, y_pred: Tensor, multiplier: float) -> Tensor:\n",
    "        # return multiplier * (1 - y_pred.max(-1)).sum(-1)\n",
    "        closeness = (1 - 4 * (y_pred - 0.5)**2)\n",
    "        return multiplier * closeness.sum(-1).sum(-1)\n",
    "        # return multiplier * closeness.sum(-1).sum(-1)\n",
    "        \n",
    "    def __call__(self) -> Callable[[Tensor, Tensor], Tensor]:\n",
    "        \"\"\" Return a loss function that is callable by keras. \"\"\"\n",
    "        def custom_loss(y_true: Tensor, y_pred: Tensor) -> Tensor:\n",
    "            exp2 = self.exponents_of_2_tensor(self.input_shape)\n",
    "            true = ((self.input_tensor % 2) * exp2).sum(-1)\n",
    "            pred = (y_pred * self.make_meshgrid(self.output_shape)).sum(-1)\n",
    "            # return ((true - pred)**2).sum(-1) + self.penalize_noncommitment(y_pred, 1)\n",
    "            return self.penalize_noncommitment(y_pred, 1)\n",
    "        return custom_loss\n",
    "    \n",
    "A = T.dtensor3(\"A\")\n",
    "B = T.dtensor3(\"B\")\n",
    "C = T.dtensor3(\"C\")\n",
    "inputs = data[0:5]\n",
    "y_true = output[0:5]\n",
    "y_pred = output[0:5]\n",
    "D = InputTensorLossFunction(data.shape[1:], output.shape[1:], A)()\n",
    "E = D(B, C)\n",
    "F = theano.function([A, B, C], E, on_unused_input=\"ignore\")\n",
    "evaluation = F(inputs, y_true, y_pred).round(2)\n",
    "print(\"=== Input ===\\n{}\".format(inputs))\n",
    "print(\"\\n=== Prediction ===\\n{}\".format(y_pred))\n",
    "print(\"\\n=== Losses ===\\n{}\".format(evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 6, 3)              0         \n",
      "_________________________________________________________________\n",
      "Dropout 1 (Dropout)          (None, 6, 3)              0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 6, 1024)           1585152   \n",
      "_________________________________________________________________\n",
      "Dropout 2 (Dropout)          (None, 6, 1024)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 6, 1024)           4721664   \n",
      "_________________________________________________________________\n",
      "Dropout 3 (Dropout)          (None, 6, 1024)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 6, 1024)           4721664   \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 6, 8)              8200      \n",
      "=================================================================\n",
      "Total params: 11,036,680\n",
      "Trainable params: 11,036,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "input_layer  = Input(name=\"Input\", shape=data.shape[1:])\n",
    "model_layer  = Dropout(name=\"Dropout 1\", rate=1/2)(input_layer)\n",
    "model_layer  = Bidirectional(GRU(name=\"GRU 1\", units=512, return_sequences=True,\n",
    "                    recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "model_layer  = Dropout(name=\"Dropout 2\", rate=1/2)(model_layer)\n",
    "model_layer  = Bidirectional(GRU(name=\"GRU 2\", units=512, return_sequences=True,\n",
    "                   recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "model_layer  = Dropout(name=\"Dropout 3\", rate=1/2)(model_layer)\n",
    "model_layer  = Bidirectional(GRU(name=\"GRU 3\", units=512, return_sequences=True,\n",
    "                   recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "output_layer = Dense(name=\"Output\", units=output.shape[2], activation=\"softmax\")(model_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "loss  = InputTensorLossFunction(data.shape[1:], output.shape[1:], input_layer)()\n",
    "opt   = keras.optimizers.RMSprop(lr=10**(-9))\n",
    "model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1024 samples, validate on 512 samples\n",
      "Epoch 1/64\n",
      "30s - loss: 20.8280 - acc: 0.1396 - val_loss: 20.8678 - val_acc: 0.1742\n",
      "Epoch 2/64\n",
      "30s - loss: 20.8225 - acc: 0.1405 - val_loss: 20.8661 - val_acc: 0.1751\n",
      "Epoch 3/64\n",
      "30s - loss: 20.8243 - acc: 0.1431 - val_loss: 20.8645 - val_acc: 0.1758\n",
      "Epoch 4/64\n",
      "30s - loss: 20.8191 - acc: 0.1413 - val_loss: 20.8628 - val_acc: 0.1758\n",
      "Epoch 5/64\n",
      "30s - loss: 20.8191 - acc: 0.1419 - val_loss: 20.8612 - val_acc: 0.1758\n",
      "Epoch 6/64\n",
      "30s - loss: 20.8217 - acc: 0.1458 - val_loss: 20.8595 - val_acc: 0.1761\n",
      "Epoch 7/64\n",
      "30s - loss: 20.8165 - acc: 0.1440 - val_loss: 20.8577 - val_acc: 0.1768\n",
      "Epoch 8/64\n",
      "30s - loss: 20.8132 - acc: 0.1423 - val_loss: 20.8560 - val_acc: 0.1768\n",
      "Epoch 9/64\n",
      "30s - loss: 20.8128 - acc: 0.1437 - val_loss: 20.8543 - val_acc: 0.1768\n",
      "Epoch 10/64\n",
      "30s - loss: 20.8168 - acc: 0.1362 - val_loss: 20.8525 - val_acc: 0.1768\n",
      "Epoch 11/64\n",
      "30s - loss: 20.8188 - acc: 0.1431 - val_loss: 20.8507 - val_acc: 0.1768\n",
      "Epoch 12/64\n",
      "30s - loss: 20.8073 - acc: 0.1341 - val_loss: 20.8489 - val_acc: 0.1768\n",
      "Epoch 13/64\n",
      "30s - loss: 20.8131 - acc: 0.1383 - val_loss: 20.8471 - val_acc: 0.1771\n",
      "Epoch 14/64\n",
      "30s - loss: 20.8079 - acc: 0.1512 - val_loss: 20.8452 - val_acc: 0.1774\n",
      "Epoch 15/64\n",
      "30s - loss: 20.8087 - acc: 0.1528 - val_loss: 20.8433 - val_acc: 0.1771\n",
      "Epoch 16/64\n",
      "30s - loss: 20.8080 - acc: 0.1489 - val_loss: 20.8414 - val_acc: 0.1771\n",
      "Epoch 17/64\n",
      "30s - loss: 20.8062 - acc: 0.1553 - val_loss: 20.8395 - val_acc: 0.1771\n",
      "Epoch 18/64\n",
      "30s - loss: 20.8073 - acc: 0.1421 - val_loss: 20.8376 - val_acc: 0.1771\n",
      "Epoch 19/64\n",
      "30s - loss: 20.8070 - acc: 0.1427 - val_loss: 20.8356 - val_acc: 0.1771\n",
      "Epoch 20/64\n",
      "30s - loss: 20.8034 - acc: 0.1536 - val_loss: 20.8337 - val_acc: 0.1771\n",
      "Epoch 21/64\n",
      "30s - loss: 20.8033 - acc: 0.1478 - val_loss: 20.8317 - val_acc: 0.1781\n",
      "Epoch 22/64\n",
      "30s - loss: 20.7965 - acc: 0.1522 - val_loss: 20.8297 - val_acc: 0.1781\n",
      "Epoch 23/64\n",
      "30s - loss: 20.7996 - acc: 0.1392 - val_loss: 20.8276 - val_acc: 0.1781\n",
      "Epoch 24/64\n",
      "30s - loss: 20.8012 - acc: 0.1473 - val_loss: 20.8256 - val_acc: 0.1784\n",
      "Epoch 25/64\n",
      "30s - loss: 20.7880 - acc: 0.1465 - val_loss: 20.8235 - val_acc: 0.1790\n",
      "Epoch 26/64\n",
      "30s - loss: 20.7939 - acc: 0.1450 - val_loss: 20.8214 - val_acc: 0.1797\n",
      "Epoch 27/64\n",
      "30s - loss: 20.7936 - acc: 0.1501 - val_loss: 20.8193 - val_acc: 0.1800\n",
      "Epoch 28/64\n",
      "30s - loss: 20.7959 - acc: 0.1445 - val_loss: 20.8172 - val_acc: 0.1810\n",
      "Epoch 29/64\n",
      "30s - loss: 20.7898 - acc: 0.1473 - val_loss: 20.8151 - val_acc: 0.1807\n",
      "Epoch 30/64\n",
      "30s - loss: 20.7887 - acc: 0.1523 - val_loss: 20.8129 - val_acc: 0.1807\n",
      "Epoch 31/64\n",
      "30s - loss: 20.7955 - acc: 0.1424 - val_loss: 20.8107 - val_acc: 0.1810\n",
      "Epoch 32/64\n",
      "30s - loss: 20.7875 - acc: 0.1418 - val_loss: 20.8085 - val_acc: 0.1807\n",
      "Epoch 33/64\n",
      "30s - loss: 20.7890 - acc: 0.1468 - val_loss: 20.8063 - val_acc: 0.1810\n",
      "Epoch 34/64\n",
      "30s - loss: 20.7841 - acc: 0.1414 - val_loss: 20.8040 - val_acc: 0.1816\n",
      "Epoch 35/64\n",
      "30s - loss: 20.7800 - acc: 0.1499 - val_loss: 20.8018 - val_acc: 0.1816\n",
      "Epoch 36/64\n",
      "30s - loss: 20.7847 - acc: 0.1530 - val_loss: 20.7995 - val_acc: 0.1823\n",
      "Epoch 37/64\n",
      "30s - loss: 20.7837 - acc: 0.1468 - val_loss: 20.7972 - val_acc: 0.1823\n",
      "Epoch 38/64\n",
      "30s - loss: 20.7756 - acc: 0.1499 - val_loss: 20.7949 - val_acc: 0.1820\n",
      "Epoch 39/64\n",
      "30s - loss: 20.7774 - acc: 0.1455 - val_loss: 20.7925 - val_acc: 0.1823\n",
      "Epoch 40/64\n",
      "30s - loss: 20.7790 - acc: 0.1515 - val_loss: 20.7901 - val_acc: 0.1823\n",
      "Epoch 41/64\n",
      "30s - loss: 20.7755 - acc: 0.1499 - val_loss: 20.7877 - val_acc: 0.1833\n",
      "Epoch 42/64\n",
      "30s - loss: 20.7718 - acc: 0.1501 - val_loss: 20.7853 - val_acc: 0.1842\n",
      "Epoch 43/64\n",
      "30s - loss: 20.7688 - acc: 0.1522 - val_loss: 20.7828 - val_acc: 0.1842\n",
      "Epoch 44/64\n",
      "30s - loss: 20.7682 - acc: 0.1450 - val_loss: 20.7804 - val_acc: 0.1846\n",
      "Epoch 45/64\n",
      "30s - loss: 20.7707 - acc: 0.1442 - val_loss: 20.7778 - val_acc: 0.1846\n",
      "Epoch 46/64\n",
      "30s - loss: 20.7569 - acc: 0.1481 - val_loss: 20.7753 - val_acc: 0.1849\n",
      "Epoch 47/64\n",
      "30s - loss: 20.7583 - acc: 0.1538 - val_loss: 20.7728 - val_acc: 0.1852\n",
      "Epoch 48/64\n",
      "30s - loss: 20.7574 - acc: 0.1561 - val_loss: 20.7702 - val_acc: 0.1852\n",
      "Epoch 49/64\n",
      "30s - loss: 20.7640 - acc: 0.1564 - val_loss: 20.7675 - val_acc: 0.1852\n",
      "Epoch 50/64\n",
      "30s - loss: 20.7565 - acc: 0.1481 - val_loss: 20.7649 - val_acc: 0.1852\n",
      "Epoch 51/64\n",
      "30s - loss: 20.7598 - acc: 0.1559 - val_loss: 20.7622 - val_acc: 0.1852\n",
      "Epoch 52/64\n",
      "30s - loss: 20.7497 - acc: 0.1538 - val_loss: 20.7596 - val_acc: 0.1865\n",
      "Epoch 53/64\n",
      "30s - loss: 20.7439 - acc: 0.1514 - val_loss: 20.7568 - val_acc: 0.1868\n",
      "Epoch 54/64\n",
      "30s - loss: 20.7459 - acc: 0.1483 - val_loss: 20.7541 - val_acc: 0.1865\n",
      "Epoch 55/64\n",
      "30s - loss: 20.7405 - acc: 0.1546 - val_loss: 20.7513 - val_acc: 0.1865\n",
      "Epoch 56/64\n",
      "30s - loss: 20.7430 - acc: 0.1602 - val_loss: 20.7485 - val_acc: 0.1862\n",
      "Epoch 57/64\n",
      "30s - loss: 20.7392 - acc: 0.1470 - val_loss: 20.7458 - val_acc: 0.1862\n",
      "Epoch 58/64\n",
      "30s - loss: 20.7392 - acc: 0.1566 - val_loss: 20.7429 - val_acc: 0.1862\n",
      "Epoch 59/64\n",
      "30s - loss: 20.7413 - acc: 0.1517 - val_loss: 20.7401 - val_acc: 0.1872\n",
      "Epoch 60/64\n",
      "30s - loss: 20.7343 - acc: 0.1475 - val_loss: 20.7372 - val_acc: 0.1872\n",
      "Epoch 61/64\n",
      "30s - loss: 20.7333 - acc: 0.1541 - val_loss: 20.7342 - val_acc: 0.1868\n",
      "Epoch 62/64\n",
      "30s - loss: 20.7362 - acc: 0.1577 - val_loss: 20.7313 - val_acc: 0.1875\n",
      "Epoch 63/64\n",
      "30s - loss: 20.7292 - acc: 0.1479 - val_loss: 20.7283 - val_acc: 0.1875\n",
      "Epoch 64/64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c18e117b649d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'################################################################################\\nnp.random.seed(1010)\\nhistories = model.fit(data, output, epochs=64, batch_size=1, verbose=2,\\n                      validation_data=(val_data, val_output),\\n                     callbacks=[keras.callbacks.ReduceLROnPlateau(\\n                         patience=5, verbose=True)])'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 989\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/jovyan/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-8.7--3.5.2-64/scan_perform/mod.cpp:6946)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/theano/gpuarray/type.py\u001b[0m in \u001b[0;36mvalue_zeros\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpygpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpuarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmay_share_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         return pygpu.gpuarray.zeros(shape, dtype=self.typecode,\n\u001b[1;32m    370\u001b[0m                                     context=self.context)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################################################################\n",
    "np.random.seed(1010)\n",
    "histories = model.fit(data, output, epochs=64, batch_size=1, verbose=2,\n",
    "                      validation_data=(val_data, val_output),\n",
    "                     callbacks=[keras.callbacks.ReduceLROnPlateau(\n",
    "                         patience=5, verbose=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "plt.plot(histories.history['loss'])\n",
    "plt.plot(histories.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "plt.plot(histories.history['acc'])\n",
    "plt.plot(histories.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "test_input  = val_data\n",
    "test_output = val_output\n",
    "predictions = model.predict(test_input)\n",
    "print(\"\\nAvg Difference: {}\".format(np.abs(test_output - predictions).mean()))\n",
    "print(\"=== Input ===\\n{}\".format(test_input))\n",
    "print(\"\\n=== True Output ===\\n{}\".format(test_output))\n",
    "print(\"\\n=== Prediction ===\\n{}\".format(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
