{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 19 14:51:39 2017       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 27%   26C    P8     9W / 180W |     10MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 1080    Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 27%   27C    P8     9W / 180W |     10MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 1080    Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 27%   25C    P8     9W / 180W |     10MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 1080    Off  | 00000000:07:00.0 Off |                  N/A |\n",
      "| 27%   24C    P8     9W / 180W |    515MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 1080    Off  | 00000000:0B:00.0 Off |                  N/A |\n",
      "| 27%   25C    P8     9W / 180W |     10MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 1080    Off  | 00000000:0C:00.0 Off |                  N/A |\n",
      "| 27%   29C    P8     9W / 180W |   7633MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 1080    Off  | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 27%   27C    P8     9W / 180W |    197MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 1080    Off  | 00000000:0E:00.0 Off |                  N/A |\n",
      "| 27%   26C    P8    10W / 180W |   8030MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 (0000:0B:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "################################################################################\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, GRU, Flatten, TimeDistributed, Dropout, LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from typing import Tuple, Callable, List, Optional, Sequence, Generator, Any\n",
    "from tracker import visuals, extractor, utils, metrics\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "np.set_printoptions(edgeitems=20)\n",
    "Tensor = theano.tensor.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Input Matrix ===\n",
      "(1024, 6, 3)\n",
      "[[[4 8 2]\n",
      "  [5 3 6]\n",
      "  [2 1 5]\n",
      "  [0 6 8]\n",
      "  [1 8 0]\n",
      "  [1 5 7]]\n",
      "\n",
      " [[6 3 8]\n",
      "  [3 5 8]\n",
      "  [6 2 5]\n",
      "  [2 5 1]\n",
      "  [7 1 5]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 3 8]\n",
      "  [8 8 8]\n",
      "  [8 7 3]\n",
      "  [6 2 3]\n",
      "  [4 6 0]\n",
      "  [4 7 2]]\n",
      "\n",
      " [[4 8 1]\n",
      "  [1 7 1]\n",
      "  [1 8 7]\n",
      "  [1 2 1]\n",
      "  [7 7 4]\n",
      "  [5 3 5]]\n",
      "\n",
      " [[5 6 6]\n",
      "  [8 0 0]\n",
      "  [2 7 7]\n",
      "  [6 1 3]\n",
      "  [6 5 8]\n",
      "  [1 2 6]]\n",
      "\n",
      " [[5 5 5]\n",
      "  [0 1 7]\n",
      "  [5 0 4]\n",
      "  [5 3 7]\n",
      "  [4 8 6]\n",
      "  [3 5 5]]\n",
      "\n",
      " [[2 6 7]\n",
      "  [0 8 6]\n",
      "  [5 2 0]\n",
      "  [4 3 8]\n",
      "  [5 7 2]\n",
      "  [3 6 6]]\n",
      "\n",
      " [[2 2 7]\n",
      "  [7 2 8]\n",
      "  [0 0 8]\n",
      "  [1 1 8]\n",
      "  [0 8 8]\n",
      "  [0 3 3]]\n",
      "\n",
      " [[2 0 6]\n",
      "  [3 6 4]\n",
      "  [2 5 0]\n",
      "  [5 0 7]\n",
      "  [5 5 4]\n",
      "  [0 8 6]]\n",
      "\n",
      " [[0 4 2]\n",
      "  [5 2 8]\n",
      "  [8 7 1]\n",
      "  [1 8 2]\n",
      "  [0 8 2]\n",
      "  [0 5 4]]\n",
      "\n",
      " [[8 6 4]\n",
      "  [7 2 1]\n",
      "  [1 8 0]\n",
      "  [2 1 3]\n",
      "  [0 6 3]\n",
      "  [7 7 2]]\n",
      "\n",
      " [[4 1 4]\n",
      "  [4 3 6]\n",
      "  [2 8 7]\n",
      "  [7 4 2]\n",
      "  [0 3 0]\n",
      "  [4 1 2]]\n",
      "\n",
      " [[5 6 2]\n",
      "  [7 6 5]\n",
      "  [5 8 8]\n",
      "  [7 6 6]\n",
      "  [8 3 7]\n",
      "  [2 2 6]]\n",
      "\n",
      " [[7 8 4]\n",
      "  [7 2 2]\n",
      "  [2 1 0]\n",
      "  [1 7 1]\n",
      "  [6 5 0]\n",
      "  [2 8 8]]\n",
      "\n",
      " [[6 4 8]\n",
      "  [3 5 8]\n",
      "  [8 2 2]\n",
      "  [4 6 7]\n",
      "  [4 2 5]\n",
      "  [3 0 3]]\n",
      "\n",
      " [[8 5 3]\n",
      "  [7 6 6]\n",
      "  [7 5 4]\n",
      "  [5 7 8]\n",
      "  [1 1 1]\n",
      "  [4 3 6]]\n",
      "\n",
      " [[0 1 0]\n",
      "  [7 8 7]\n",
      "  [8 3 4]\n",
      "  [4 6 7]\n",
      "  [5 4 8]\n",
      "  [1 0 5]]\n",
      "\n",
      " [[4 6 3]\n",
      "  [3 2 0]\n",
      "  [8 1 6]\n",
      "  [7 0 5]\n",
      "  [5 0 4]\n",
      "  [2 3 6]]\n",
      "\n",
      " [[5 6 6]\n",
      "  [4 2 6]\n",
      "  [0 4 0]\n",
      "  [8 5 3]\n",
      "  [2 3 3]\n",
      "  [5 5 8]]\n",
      "\n",
      " [[0 2 6]\n",
      "  [7 7 7]\n",
      "  [7 5 8]\n",
      "  [8 4 5]\n",
      "  [0 2 5]\n",
      "  [3 0 8]]\n",
      "\n",
      " ..., \n",
      " [[8 2 1]\n",
      "  [4 1 7]\n",
      "  [1 0 0]\n",
      "  [6 5 1]\n",
      "  [5 4 2]\n",
      "  [0 3 7]]\n",
      "\n",
      " [[1 5 5]\n",
      "  [2 6 4]\n",
      "  [0 2 3]\n",
      "  [5 7 8]\n",
      "  [4 4 1]\n",
      "  [3 1 8]]\n",
      "\n",
      " [[5 7 5]\n",
      "  [0 6 8]\n",
      "  [3 2 5]\n",
      "  [3 6 7]\n",
      "  [5 0 3]\n",
      "  [5 2 3]]\n",
      "\n",
      " [[5 0 1]\n",
      "  [4 8 5]\n",
      "  [7 6 2]\n",
      "  [5 0 8]\n",
      "  [5 5 1]\n",
      "  [6 1 7]]\n",
      "\n",
      " [[8 3 8]\n",
      "  [3 7 3]\n",
      "  [0 5 6]\n",
      "  [5 5 0]\n",
      "  [6 1 5]\n",
      "  [8 4 3]]\n",
      "\n",
      " [[0 2 6]\n",
      "  [8 5 5]\n",
      "  [6 6 0]\n",
      "  [2 4 0]\n",
      "  [2 5 7]\n",
      "  [2 0 1]]\n",
      "\n",
      " [[8 4 5]\n",
      "  [1 8 1]\n",
      "  [5 0 7]\n",
      "  [8 5 2]\n",
      "  [3 8 4]\n",
      "  [7 2 7]]\n",
      "\n",
      " [[2 6 7]\n",
      "  [0 0 3]\n",
      "  [8 3 2]\n",
      "  [6 7 0]\n",
      "  [8 4 7]\n",
      "  [0 8 4]]\n",
      "\n",
      " [[0 3 5]\n",
      "  [4 5 8]\n",
      "  [8 8 2]\n",
      "  [3 2 4]\n",
      "  [1 5 8]\n",
      "  [8 8 2]]\n",
      "\n",
      " [[7 3 6]\n",
      "  [5 3 6]\n",
      "  [4 0 7]\n",
      "  [7 0 6]\n",
      "  [1 8 3]\n",
      "  [4 0 6]]\n",
      "\n",
      " [[4 8 6]\n",
      "  [0 6 5]\n",
      "  [2 2 2]\n",
      "  [6 5 5]\n",
      "  [1 1 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " [[4 3 1]\n",
      "  [2 8 8]\n",
      "  [1 5 5]\n",
      "  [0 5 4]\n",
      "  [6 2 5]\n",
      "  [0 7 2]]\n",
      "\n",
      " [[1 0 4]\n",
      "  [1 0 6]\n",
      "  [2 5 5]\n",
      "  [8 7 5]\n",
      "  [4 1 4]\n",
      "  [3 7 0]]\n",
      "\n",
      " [[5 5 8]\n",
      "  [1 0 0]\n",
      "  [3 6 2]\n",
      "  [3 6 3]\n",
      "  [8 3 4]\n",
      "  [7 0 5]]\n",
      "\n",
      " [[7 6 3]\n",
      "  [3 3 7]\n",
      "  [5 4 7]\n",
      "  [8 7 7]\n",
      "  [5 2 5]\n",
      "  [1 2 3]]\n",
      "\n",
      " [[7 8 4]\n",
      "  [2 8 8]\n",
      "  [5 1 7]\n",
      "  [4 0 5]\n",
      "  [3 8 6]\n",
      "  [6 1 7]]\n",
      "\n",
      " [[8 0 5]\n",
      "  [3 4 1]\n",
      "  [6 4 0]\n",
      "  [4 7 6]\n",
      "  [7 4 8]\n",
      "  [7 0 0]]\n",
      "\n",
      " [[4 0 8]\n",
      "  [8 1 4]\n",
      "  [5 3 2]\n",
      "  [7 8 6]\n",
      "  [3 3 6]\n",
      "  [7 6 6]]\n",
      "\n",
      " [[3 3 2]\n",
      "  [7 0 6]\n",
      "  [7 0 5]\n",
      "  [4 8 5]\n",
      "  [3 2 8]\n",
      "  [7 5 4]]\n",
      "\n",
      " [[7 2 1]\n",
      "  [3 8 3]\n",
      "  [4 7 3]\n",
      "  [5 5 5]\n",
      "  [6 1 5]\n",
      "  [6 4 6]]]\n",
      "\n",
      "=== Output Matrix ===\n",
      "(1024, 6, 8)\n",
      "[[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " ..., \n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "def get_data(\n",
    "        num_vectors : int,\n",
    "        vector_size : int = 3,\n",
    "        min_value   : int = 0,\n",
    "        max_value   : int = 9,\n",
    "        ) -> np.ndarray:\n",
    "    return np.random.randint(min_value, max_value, (num_vectors, vector_size))\n",
    "\n",
    "def get_output(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a 1-hot categorical matrix from *data*.\n",
    "    *data* is a 2D array. Create output array by:\n",
    "    For each cell in *data*, transform to 1 if cell is even. Else, 0.\n",
    "    For each row, concatenate all the 1s and 0s into a single binary number.\n",
    "    The binary number corresponds to that row's category.\n",
    "    Example: If row = (2, 4, 80), then category = 000 = 0.\n",
    "    Example: If row = (1, 30, 9), then category = 101 = 5.\n",
    "    Example: If row = (3, 9, 12, 7), then category = 1101 = 13.\n",
    "    Example: If row = (2, 1), then category = 01 = 1.\n",
    "    \"\"\"\n",
    "    weights = np.array([2**i for i in range(data.shape[1])])[::-1]\n",
    "    output = np.zeros((len(data), 2**len(weights)))\n",
    "    indices = (data % 2) @ weights\n",
    "    output[np.arange(len(data)), indices] = 1\n",
    "    return output\n",
    "\n",
    "np.random.seed(1010)\n",
    "data   = np.array([get_data(num_vectors=6) for _ in range(1024)])\n",
    "output = np.array([get_output(matrix) for matrix in data])\n",
    "val_data   = np.array([get_data(num_vectors=6) for _ in range(512)])\n",
    "val_output = np.array([get_output(matrix) for matrix in val_data])\n",
    "print(\"=== Input Matrix ===\\n{0}\\n{1}\".format(data.shape, data))\n",
    "print(\"\\n=== Output Matrix ===\\n{0}\\n{1}\".format(output.shape, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss is purely the built-in categorical cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 6, 3)              0         \n",
      "_________________________________________________________________\n",
      "Dropout 1 (Dropout)          (None, 6, 3)              0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 6, 1024)           2113536   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 1024)           4096      \n",
      "_________________________________________________________________\n",
      "Dropout 2 (Dropout)          (None, 6, 1024)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 6, 1024)           6295552   \n",
      "_________________________________________________________________\n",
      "Dropout 3 (Dropout)          (None, 6, 1024)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 6, 1024)           6295552   \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 6, 8)              8200      \n",
      "=================================================================\n",
      "Total params: 14,716,936\n",
      "Trainable params: 14,714,888\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "input_layer  = Input(name=\"Input\", shape=data.shape[1:])\n",
    "model_layer  = Dropout(name=\"Dropout 1\", rate=1/2)(input_layer)\n",
    "model_layer  = Bidirectional(LSTM(name=\"GRU 1\", units=512, return_sequences=True,\n",
    "                    recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "model_layer  = keras.layers.BatchNormalization()(model_layer)\n",
    "model_layer  = Dropout(name=\"Dropout 2\", rate=1/2)(model_layer)\n",
    "model_layer  = Bidirectional(LSTM(name=\"GRU 2\", units=512, return_sequences=True,\n",
    "                   recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "model_layer  = Dropout(name=\"Dropout 3\", rate=1/2)(model_layer)\n",
    "model_layer  = Bidirectional(LSTM(name=\"GRU 3\", units=512, return_sequences=True,\n",
    "                   recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "output_layer = Dense(name=\"Output\", units=output.shape[2], activation=\"softmax\")(model_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1024 samples, validate on 512 samples\n",
      "Epoch 1/64\n",
      "23s - loss: 2.1154 - acc: 0.1571 - val_loss: 2.0603 - val_acc: 0.1826\n",
      "Epoch 2/64\n",
      "23s - loss: 2.0800 - acc: 0.1579 - val_loss: 2.0724 - val_acc: 0.1663\n",
      "Epoch 3/64\n",
      "23s - loss: 2.0772 - acc: 0.1660 - val_loss: 2.0681 - val_acc: 0.1878\n",
      "Epoch 4/64\n",
      "23s - loss: 2.0810 - acc: 0.1580 - val_loss: 2.0737 - val_acc: 0.1790\n",
      "Epoch 5/64\n",
      "23s - loss: 2.0809 - acc: 0.1608 - val_loss: 2.0827 - val_acc: 0.1156\n",
      "Epoch 6/64\n",
      "23s - loss: 2.0851 - acc: 0.1558 - val_loss: 2.0784 - val_acc: 0.1305\n",
      "Epoch 7/64\n",
      "23s - loss: 2.0847 - acc: 0.1551 - val_loss: 2.1034 - val_acc: 0.1338\n",
      "Epoch 8/64\n",
      "23s - loss: 2.0914 - acc: 0.1580 - val_loss: 2.0799 - val_acc: 0.1732\n",
      "Epoch 9/64\n",
      "23s - loss: 2.0930 - acc: 0.1567 - val_loss: 2.0679 - val_acc: 0.1878\n",
      "Epoch 10/64\n",
      "24s - loss: 2.0922 - acc: 0.1527 - val_loss: 2.0792 - val_acc: 0.1374\n",
      "Epoch 11/64\n",
      "25s - loss: 2.0936 - acc: 0.1553 - val_loss: 2.1158 - val_acc: 0.1312\n",
      "Epoch 12/64\n",
      "24s - loss: 2.0993 - acc: 0.1488 - val_loss: 2.0642 - val_acc: 0.1875\n",
      "Epoch 13/64\n",
      "24s - loss: 2.0958 - acc: 0.1528 - val_loss: 2.1006 - val_acc: 0.1413\n",
      "Epoch 14/64\n",
      "24s - loss: 2.0936 - acc: 0.1587 - val_loss: 2.0685 - val_acc: 0.1878\n",
      "Epoch 15/64\n",
      "24s - loss: 2.1003 - acc: 0.1486 - val_loss: 2.1096 - val_acc: 0.1318\n",
      "Epoch 16/64\n",
      "24s - loss: 2.1002 - acc: 0.1497 - val_loss: 2.0876 - val_acc: 0.1396\n",
      "Epoch 17/64\n",
      "24s - loss: 2.1024 - acc: 0.1465 - val_loss: 2.0684 - val_acc: 0.1878\n",
      "Epoch 18/64\n",
      "24s - loss: 2.1000 - acc: 0.1512 - val_loss: 2.1327 - val_acc: 0.1071\n",
      "Epoch 19/64\n",
      "24s - loss: 2.1061 - acc: 0.1473 - val_loss: 2.1126 - val_acc: 0.1878\n",
      "Epoch 20/64\n",
      "24s - loss: 2.1074 - acc: 0.1561 - val_loss: 2.0852 - val_acc: 0.1133\n",
      "Epoch 21/64\n",
      "24s - loss: 2.1071 - acc: 0.1408 - val_loss: 2.0825 - val_acc: 0.1878\n",
      "Epoch 22/64\n",
      "24s - loss: 2.1076 - acc: 0.1512 - val_loss: 2.1001 - val_acc: 0.1172\n",
      "Epoch 23/64\n",
      "24s - loss: 2.1065 - acc: 0.1484 - val_loss: 2.1135 - val_acc: 0.1878\n",
      "Epoch 24/64\n",
      "24s - loss: 2.1067 - acc: 0.1453 - val_loss: 2.1435 - val_acc: 0.1032\n",
      "Epoch 25/64\n",
      "24s - loss: 2.1094 - acc: 0.1501 - val_loss: 2.0945 - val_acc: 0.1722\n",
      "Epoch 26/64\n",
      "24s - loss: 2.1154 - acc: 0.1423 - val_loss: 2.1155 - val_acc: 0.1097\n",
      "Epoch 27/64\n",
      "24s - loss: 2.1159 - acc: 0.1471 - val_loss: 2.1404 - val_acc: 0.1032\n",
      "Epoch 28/64\n",
      "24s - loss: 2.1132 - acc: 0.1466 - val_loss: 2.1489 - val_acc: 0.1387\n",
      "Epoch 29/64\n",
      "24s - loss: 2.1164 - acc: 0.1449 - val_loss: 2.1049 - val_acc: 0.1328\n",
      "Epoch 30/64\n",
      "24s - loss: 2.1146 - acc: 0.1465 - val_loss: 2.1048 - val_acc: 0.1416\n",
      "Epoch 31/64\n",
      "24s - loss: 2.1101 - acc: 0.1479 - val_loss: 2.1282 - val_acc: 0.1032\n",
      "Epoch 32/64\n",
      "24s - loss: 2.1129 - acc: 0.1507 - val_loss: 2.0793 - val_acc: 0.1523\n",
      "Epoch 33/64\n",
      "24s - loss: 2.1126 - acc: 0.1540 - val_loss: 2.1024 - val_acc: 0.1641\n",
      "Epoch 34/64\n",
      "24s - loss: 2.1202 - acc: 0.1431 - val_loss: 2.1314 - val_acc: 0.1208\n",
      "Epoch 35/64\n",
      "24s - loss: 2.1180 - acc: 0.1501 - val_loss: 2.1398 - val_acc: 0.1357\n",
      "Epoch 36/64\n",
      "24s - loss: 2.1240 - acc: 0.1426 - val_loss: 2.0773 - val_acc: 0.1823\n",
      "Epoch 37/64\n",
      "24s - loss: 2.1172 - acc: 0.1466 - val_loss: 2.1048 - val_acc: 0.1878\n",
      "Epoch 38/64\n",
      "24s - loss: 2.1227 - acc: 0.1439 - val_loss: 2.0940 - val_acc: 0.1878\n",
      "Epoch 39/64\n",
      "24s - loss: 2.1213 - acc: 0.1481 - val_loss: 2.0801 - val_acc: 0.1494\n",
      "Epoch 40/64\n",
      "24s - loss: 2.1192 - acc: 0.1548 - val_loss: 2.0956 - val_acc: 0.1136\n",
      "Epoch 41/64\n",
      "24s - loss: 2.1256 - acc: 0.1447 - val_loss: 2.0951 - val_acc: 0.1777\n",
      "Epoch 42/64\n",
      "24s - loss: 2.1248 - acc: 0.1447 - val_loss: 2.0598 - val_acc: 0.1878\n",
      "Epoch 43/64\n",
      "24s - loss: 2.1239 - acc: 0.1471 - val_loss: 2.1704 - val_acc: 0.1878\n",
      "Epoch 44/64\n",
      "23s - loss: 2.1322 - acc: 0.1434 - val_loss: 2.0873 - val_acc: 0.1878\n",
      "Epoch 45/64\n",
      "23s - loss: 2.1242 - acc: 0.1536 - val_loss: 2.0906 - val_acc: 0.1396\n",
      "Epoch 46/64\n",
      "23s - loss: 2.1316 - acc: 0.1424 - val_loss: 2.0923 - val_acc: 0.1423\n",
      "Epoch 47/64\n",
      "23s - loss: 2.1267 - acc: 0.1419 - val_loss: 2.1072 - val_acc: 0.1878\n",
      "Epoch 48/64\n",
      "22s - loss: 2.1320 - acc: 0.1462 - val_loss: 2.1527 - val_acc: 0.1396\n",
      "Epoch 49/64\n",
      "22s - loss: 2.1272 - acc: 0.1466 - val_loss: 2.0897 - val_acc: 0.1361\n",
      "Epoch 50/64\n",
      "22s - loss: 2.1313 - acc: 0.1423 - val_loss: 2.1469 - val_acc: 0.1084\n",
      "Epoch 51/64\n",
      "22s - loss: 2.1275 - acc: 0.1475 - val_loss: 2.1445 - val_acc: 0.1331\n",
      "Epoch 52/64\n",
      "22s - loss: 2.1330 - acc: 0.1396 - val_loss: 2.2276 - val_acc: 0.0964\n",
      "Epoch 53/64\n",
      "22s - loss: 2.1304 - acc: 0.1481 - val_loss: 2.1169 - val_acc: 0.1120\n",
      "Epoch 54/64\n",
      "22s - loss: 2.1400 - acc: 0.1442 - val_loss: 2.1244 - val_acc: 0.1032\n",
      "Epoch 55/64\n",
      "22s - loss: 2.1331 - acc: 0.1499 - val_loss: 2.2144 - val_acc: 0.1878\n",
      "Epoch 56/64\n",
      "22s - loss: 2.1392 - acc: 0.1410 - val_loss: 2.0954 - val_acc: 0.1878\n",
      "Epoch 57/64\n",
      "22s - loss: 2.1415 - acc: 0.1349 - val_loss: 2.1438 - val_acc: 0.1136\n",
      "Epoch 58/64\n",
      "22s - loss: 2.1372 - acc: 0.1457 - val_loss: 2.1925 - val_acc: 0.1312\n",
      "Epoch 59/64\n",
      "22s - loss: 2.1391 - acc: 0.1405 - val_loss: 2.1714 - val_acc: 0.1878\n",
      "Epoch 60/64\n",
      "22s - loss: 2.1407 - acc: 0.1431 - val_loss: 2.1338 - val_acc: 0.1413\n",
      "Epoch 61/64\n",
      "22s - loss: 2.1367 - acc: 0.1418 - val_loss: 2.1243 - val_acc: 0.1374\n",
      "Epoch 62/64\n",
      "22s - loss: 2.1405 - acc: 0.1403 - val_loss: 2.1334 - val_acc: 0.0964\n",
      "Epoch 63/64\n",
      "22s - loss: 2.1424 - acc: 0.1450 - val_loss: 2.1895 - val_acc: 0.1312\n",
      "Epoch 64/64\n",
      "22s - loss: 2.1418 - acc: 0.1457 - val_loss: 2.1613 - val_acc: 0.1009\n",
      "CPU times: user 28min 20s, sys: 1.52 s, total: 28min 22s\n",
      "Wall time: 28min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################################################################\n",
    "np.random.seed(1010)\n",
    "histories = model.fit(data, output, epochs=64, batch_size=1, verbose=2,\n",
    "                      validation_data=(val_data, val_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8e31e2400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwszMKrlYmJlZJRcLMzOr9DdxKEBhnHu4jwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe8e706ac88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################################################\n",
    "plt.plot(histories.history['loss'])\n",
    "plt.plot(histories.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "plt.plot(histories.history['acc'])\n",
    "plt.plot(histories.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Difference: 0.21354736322670456\n",
      "=== Input ===\n",
      "[[[7 0 5]\n",
      "  [7 5 0]\n",
      "  [0 2 3]\n",
      "  [0 3 1]\n",
      "  [8 8 8]\n",
      "  [4 4 7]]\n",
      "\n",
      " [[0 3 5]\n",
      "  [0 4 3]\n",
      "  [1 8 5]\n",
      "  [6 2 4]\n",
      "  [3 6 2]\n",
      "  [2 3 8]]\n",
      "\n",
      " [[7 4 2]\n",
      "  [4 1 6]\n",
      "  [2 3 4]\n",
      "  [6 5 5]\n",
      "  [8 6 4]\n",
      "  [2 3 1]]\n",
      "\n",
      " [[8 0 1]\n",
      "  [8 0 5]\n",
      "  [8 6 7]\n",
      "  [2 0 2]\n",
      "  [2 1 8]\n",
      "  [7 3 0]]\n",
      "\n",
      " [[6 5 1]\n",
      "  [7 4 8]\n",
      "  [7 1 0]\n",
      "  [8 6 8]\n",
      "  [0 5 4]\n",
      "  [5 3 4]]\n",
      "\n",
      " [[2 1 6]\n",
      "  [6 8 6]\n",
      "  [7 3 7]\n",
      "  [4 8 1]\n",
      "  [3 0 2]\n",
      "  [6 2 7]]\n",
      "\n",
      " [[6 0 7]\n",
      "  [8 1 6]\n",
      "  [6 4 5]\n",
      "  [8 6 2]\n",
      "  [4 4 2]\n",
      "  [8 2 2]]\n",
      "\n",
      " [[3 2 3]\n",
      "  [5 4 2]\n",
      "  [2 4 5]\n",
      "  [7 2 6]\n",
      "  [5 1 6]\n",
      "  [3 5 7]]\n",
      "\n",
      " [[3 5 6]\n",
      "  [2 5 6]\n",
      "  [3 1 6]\n",
      "  [4 5 5]\n",
      "  [4 8 4]\n",
      "  [6 4 6]]\n",
      "\n",
      " [[8 6 8]\n",
      "  [2 6 8]\n",
      "  [1 6 0]\n",
      "  [4 5 1]\n",
      "  [6 1 2]\n",
      "  [7 3 4]]\n",
      "\n",
      " [[2 4 2]\n",
      "  [0 7 5]\n",
      "  [5 8 4]\n",
      "  [3 8 3]\n",
      "  [7 7 4]\n",
      "  [1 1 7]]\n",
      "\n",
      " [[5 7 7]\n",
      "  [4 8 5]\n",
      "  [6 4 1]\n",
      "  [7 0 0]\n",
      "  [6 7 8]\n",
      "  [5 1 0]]\n",
      "\n",
      " [[5 3 0]\n",
      "  [3 8 3]\n",
      "  [8 7 7]\n",
      "  [8 3 7]\n",
      "  [0 2 2]\n",
      "  [8 1 8]]\n",
      "\n",
      " [[0 6 4]\n",
      "  [1 3 5]\n",
      "  [1 2 3]\n",
      "  [1 5 6]\n",
      "  [7 2 0]\n",
      "  [6 0 4]]\n",
      "\n",
      " [[2 1 4]\n",
      "  [8 8 1]\n",
      "  [6 2 4]\n",
      "  [0 6 3]\n",
      "  [3 3 0]\n",
      "  [7 8 6]]\n",
      "\n",
      " [[1 4 0]\n",
      "  [1 0 7]\n",
      "  [5 3 8]\n",
      "  [3 1 4]\n",
      "  [0 6 0]\n",
      "  [3 2 2]]\n",
      "\n",
      " [[8 3 8]\n",
      "  [7 3 4]\n",
      "  [1 3 6]\n",
      "  [8 8 7]\n",
      "  [7 4 3]\n",
      "  [8 2 5]]\n",
      "\n",
      " [[2 6 8]\n",
      "  [2 1 5]\n",
      "  [5 6 6]\n",
      "  [3 5 1]\n",
      "  [5 1 5]\n",
      "  [2 4 4]]\n",
      "\n",
      " [[2 4 8]\n",
      "  [0 1 6]\n",
      "  [6 4 6]\n",
      "  [8 6 8]\n",
      "  [5 8 0]\n",
      "  [2 8 8]]\n",
      "\n",
      " [[5 4 7]\n",
      "  [0 4 0]\n",
      "  [8 2 2]\n",
      "  [1 5 0]\n",
      "  [8 1 2]\n",
      "  [4 4 5]]\n",
      "\n",
      " ..., \n",
      " [[0 7 5]\n",
      "  [4 8 3]\n",
      "  [7 3 4]\n",
      "  [3 1 1]\n",
      "  [0 3 8]\n",
      "  [4 2 8]]\n",
      "\n",
      " [[4 7 5]\n",
      "  [0 3 5]\n",
      "  [6 4 5]\n",
      "  [1 1 1]\n",
      "  [0 8 4]\n",
      "  [6 3 5]]\n",
      "\n",
      " [[0 4 0]\n",
      "  [1 7 7]\n",
      "  [4 3 3]\n",
      "  [6 6 4]\n",
      "  [0 3 8]\n",
      "  [0 4 6]]\n",
      "\n",
      " [[8 6 4]\n",
      "  [2 7 8]\n",
      "  [5 3 8]\n",
      "  [7 3 5]\n",
      "  [0 8 4]\n",
      "  [3 5 3]]\n",
      "\n",
      " [[2 1 8]\n",
      "  [2 1 4]\n",
      "  [8 1 7]\n",
      "  [4 8 7]\n",
      "  [8 8 2]\n",
      "  [2 1 1]]\n",
      "\n",
      " [[6 5 6]\n",
      "  [4 7 2]\n",
      "  [5 4 4]\n",
      "  [3 4 5]\n",
      "  [4 5 7]\n",
      "  [5 5 0]]\n",
      "\n",
      " [[7 8 1]\n",
      "  [6 2 6]\n",
      "  [2 4 8]\n",
      "  [5 8 2]\n",
      "  [4 4 4]\n",
      "  [4 1 8]]\n",
      "\n",
      " [[1 2 7]\n",
      "  [3 5 7]\n",
      "  [6 3 0]\n",
      "  [1 6 8]\n",
      "  [5 4 2]\n",
      "  [3 2 5]]\n",
      "\n",
      " [[7 4 2]\n",
      "  [0 4 4]\n",
      "  [3 1 4]\n",
      "  [8 6 0]\n",
      "  [4 4 8]\n",
      "  [1 1 8]]\n",
      "\n",
      " [[7 5 1]\n",
      "  [7 6 7]\n",
      "  [6 7 6]\n",
      "  [7 0 6]\n",
      "  [6 1 2]\n",
      "  [1 1 1]]\n",
      "\n",
      " [[5 3 8]\n",
      "  [6 2 8]\n",
      "  [0 8 8]\n",
      "  [0 3 8]\n",
      "  [4 6 5]\n",
      "  [3 5 1]]\n",
      "\n",
      " [[5 8 1]\n",
      "  [8 2 6]\n",
      "  [8 6 4]\n",
      "  [6 2 0]\n",
      "  [1 8 0]\n",
      "  [0 2 7]]\n",
      "\n",
      " [[0 8 1]\n",
      "  [0 3 7]\n",
      "  [1 3 5]\n",
      "  [2 4 0]\n",
      "  [7 5 0]\n",
      "  [1 7 0]]\n",
      "\n",
      " [[1 6 5]\n",
      "  [3 0 3]\n",
      "  [5 2 0]\n",
      "  [6 7 1]\n",
      "  [3 7 4]\n",
      "  [2 0 5]]\n",
      "\n",
      " [[5 4 3]\n",
      "  [2 7 0]\n",
      "  [4 5 0]\n",
      "  [5 5 0]\n",
      "  [4 8 8]\n",
      "  [8 0 3]]\n",
      "\n",
      " [[8 8 8]\n",
      "  [0 1 7]\n",
      "  [2 1 6]\n",
      "  [8 0 5]\n",
      "  [0 4 6]\n",
      "  [8 1 2]]\n",
      "\n",
      " [[3 6 4]\n",
      "  [4 2 4]\n",
      "  [7 2 1]\n",
      "  [4 3 4]\n",
      "  [8 1 1]\n",
      "  [8 3 5]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [4 1 0]\n",
      "  [8 8 1]\n",
      "  [5 4 8]\n",
      "  [0 8 2]\n",
      "  [0 0 2]]\n",
      "\n",
      " [[8 4 7]\n",
      "  [0 8 0]\n",
      "  [2 2 6]\n",
      "  [1 2 5]\n",
      "  [6 4 2]\n",
      "  [5 3 4]]\n",
      "\n",
      " [[4 2 0]\n",
      "  [4 5 7]\n",
      "  [5 6 5]\n",
      "  [0 0 3]\n",
      "  [8 5 8]\n",
      "  [5 8 1]]]\n",
      "\n",
      "=== True Output ===\n",
      "[[[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " ..., \n",
      " [[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "\n",
      " [[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]]]\n",
      "\n",
      "=== Prediction ===\n",
      "[[[ 0.27  0.1   0.18  0.07  0.15  0.11  0.1   0.04]\n",
      "  [ 0.17  0.11  0.18  0.05  0.2   0.09  0.17  0.03]\n",
      "  [ 0.02  0.05  0.16  0.35  0.02  0.06  0.11  0.22]\n",
      "  [ 0.06  0.34  0.02  0.08  0.06  0.35  0.01  0.08]\n",
      "  [ 0.12  0.2   0.06  0.12  0.12  0.27  0.04  0.07]\n",
      "  [ 0.14  0.06  0.23  0.13  0.11  0.11  0.15  0.07]]\n",
      "\n",
      " [[ 0.24  0.18  0.05  0.05  0.2   0.18  0.06  0.05]\n",
      "  [ 0.09  0.28  0.02  0.06  0.13  0.32  0.02  0.06]\n",
      "  [ 0.04  0.06  0.01  0.02  0.32  0.46  0.04  0.06]\n",
      "  [ 0.13  0.22  0.05  0.11  0.12  0.29  0.03  0.07]\n",
      "  [ 0.01  0.11  0.02  0.16  0.03  0.3   0.03  0.34]\n",
      "  [ 0.06  0.06  0.03  0.04  0.24  0.4   0.06  0.11]]\n",
      "\n",
      " [[ 0.09  0.26  0.07  0.15  0.06  0.2   0.07  0.11]\n",
      "  [ 0.04  0.03  0.32  0.21  0.04  0.04  0.22  0.1 ]\n",
      "  [ 0.03  0.05  0.01  0.01  0.28  0.51  0.04  0.06]\n",
      "  [ 0.13  0.18  0.09  0.11  0.13  0.24  0.05  0.07]\n",
      "  [ 0.1   0.13  0.12  0.17  0.09  0.19  0.08  0.12]\n",
      "  [ 0.02  0.07  0.01  0.03  0.11  0.59  0.03  0.15]]\n",
      "\n",
      " [[ 0.1   0.22  0.09  0.18  0.07  0.16  0.07  0.11]\n",
      "  [ 0.16  0.11  0.2   0.11  0.14  0.11  0.11  0.05]\n",
      "  [ 0.2   0.15  0.11  0.05  0.23  0.17  0.07  0.03]\n",
      "  [ 0.01  0.05  0.01  0.05  0.06  0.44  0.04  0.35]\n",
      "  [ 0.01  0.01  0.08  0.05  0.07  0.08  0.32  0.38]\n",
      "  [ 0.01  0.01  0.35  0.2   0.01  0.01  0.3   0.12]]\n",
      "\n",
      " [[ 0.07  0.16  0.16  0.17  0.04  0.1   0.15  0.14]\n",
      "  [ 0.2   0.12  0.18  0.08  0.14  0.12  0.11  0.04]\n",
      "  [ 0.04  0.03  0.39  0.18  0.03  0.03  0.21  0.08]\n",
      "  [ 0.1   0.11  0.14  0.12  0.15  0.19  0.1   0.09]\n",
      "  [ 0.1   0.17  0.15  0.21  0.07  0.14  0.05  0.11]\n",
      "  [ 0.23  0.14  0.11  0.08  0.17  0.16  0.06  0.04]]\n",
      "\n",
      " [[ 0.01  0.01  0.15  0.09  0.05  0.03  0.41  0.24]\n",
      "  [ 0.23  0.17  0.08  0.04  0.25  0.16  0.05  0.02]\n",
      "  [ 0.25  0.15  0.09  0.05  0.24  0.15  0.05  0.02]\n",
      "  [ 0.06  0.32  0.02  0.08  0.08  0.34  0.02  0.08]\n",
      "  [ 0.01  0.1   0.02  0.2   0.04  0.22  0.04  0.37]\n",
      "  [ 0.03  0.02  0.35  0.21  0.03  0.03  0.19  0.15]]\n",
      "\n",
      " [[ 0.2   0.11  0.19  0.09  0.15  0.11  0.11  0.05]\n",
      "  [ 0.02  0.01  0.4   0.17  0.02  0.01  0.29  0.08]\n",
      "  [ 0.19  0.15  0.1   0.05  0.24  0.17  0.07  0.03]\n",
      "  [ 0.04  0.25  0.04  0.18  0.06  0.26  0.03  0.14]\n",
      "  [ 0.05  0.28  0.03  0.15  0.06  0.3   0.02  0.12]\n",
      "  [ 0.01  0.03  0.13  0.41  0.01  0.04  0.1   0.27]]\n",
      "\n",
      " [[ 0.01  0.02  0.16  0.26  0.02  0.03  0.22  0.28]\n",
      "  [ 0.07  0.32  0.04  0.11  0.08  0.28  0.02  0.07]\n",
      "  [ 0.03  0.04  0.02  0.02  0.28  0.42  0.06  0.12]\n",
      "  [ 0.1   0.08  0.22  0.19  0.09  0.11  0.1   0.11]\n",
      "  [ 0.01  0.01  0.3   0.27  0.02  0.02  0.16  0.2 ]\n",
      "  [ 0.02  0.03  0.08  0.09  0.09  0.21  0.19  0.29]]\n",
      "\n",
      " [[ 0.1   0.06  0.12  0.06  0.2   0.15  0.2   0.11]\n",
      "  [ 0.04  0.03  0.06  0.03  0.3   0.22  0.19  0.13]\n",
      "  [ 0.04  0.04  0.1   0.09  0.2   0.22  0.17  0.15]\n",
      "  [ 0.04  0.12  0.06  0.13  0.1   0.3   0.07  0.19]\n",
      "  [ 0.06  0.23  0.02  0.1   0.08  0.42  0.02  0.09]\n",
      "  [ 0.13  0.08  0.21  0.18  0.09  0.11  0.12  0.09]]\n",
      "\n",
      " [[ 0.18  0.11  0.15  0.08  0.15  0.11  0.17  0.06]\n",
      "  [ 0.07  0.04  0.05  0.02  0.4   0.2   0.16  0.06]\n",
      "  [ 0.05  0.04  0.06  0.02  0.37  0.15  0.21  0.09]\n",
      "  [ 0.05  0.2   0.08  0.21  0.05  0.16  0.05  0.19]\n",
      "  [ 0.04  0.14  0.08  0.33  0.04  0.13  0.05  0.19]\n",
      "  [ 0.17  0.19  0.08  0.11  0.14  0.21  0.05  0.07]]\n",
      "\n",
      " [[ 0.03  0.1   0.02  0.05  0.15  0.36  0.08  0.21]\n",
      "  [ 0.17  0.18  0.06  0.05  0.26  0.19  0.05  0.04]\n",
      "  [ 0.13  0.25  0.03  0.05  0.19  0.29  0.02  0.03]\n",
      "  [ 0.04  0.21  0.01  0.05  0.1   0.5   0.01  0.07]\n",
      "  [ 0.07  0.15  0.09  0.21  0.08  0.18  0.05  0.16]\n",
      "  [ 0.    0.    0.1   0.08  0.04  0.05  0.33  0.39]]\n",
      "\n",
      " [[ 0.2   0.11  0.16  0.07  0.16  0.09  0.17  0.05]\n",
      "  [ 0.21  0.14  0.06  0.03  0.28  0.18  0.06  0.02]\n",
      "  [ 0.06  0.23  0.09  0.21  0.05  0.15  0.05  0.16]\n",
      "  [ 0.14  0.15  0.18  0.12  0.14  0.09  0.11  0.08]\n",
      "  [ 0.07  0.1   0.16  0.19  0.07  0.12  0.1   0.2 ]\n",
      "  [ 0.01  0.01  0.33  0.2   0.01  0.01  0.3   0.14]]\n",
      "\n",
      " [[ 0.26  0.14  0.12  0.04  0.19  0.09  0.13  0.03]\n",
      "  [ 0.11  0.17  0.03  0.04  0.26  0.29  0.05  0.04]\n",
      "  [ 0.15  0.13  0.14  0.08  0.22  0.14  0.1   0.05]\n",
      "  [ 0.24  0.13  0.08  0.05  0.26  0.16  0.05  0.03]\n",
      "  [ 0.04  0.22  0.04  0.23  0.04  0.2   0.03  0.19]\n",
      "  [ 0.01  0.01  0.38  0.21  0.02  0.01  0.24  0.12]]\n",
      "\n",
      " [[ 0.11  0.11  0.13  0.1   0.16  0.1   0.14  0.16]\n",
      "  [ 0.05  0.04  0.02  0.02  0.43  0.3   0.07  0.08]\n",
      "  [ 0.    0.01  0.02  0.11  0.01  0.05  0.11  0.69]\n",
      "  [ 0.02  0.04  0.02  0.03  0.25  0.35  0.07  0.23]\n",
      "  [ 0.02  0.02  0.3   0.27  0.02  0.01  0.22  0.14]\n",
      "  [ 0.08  0.15  0.13  0.25  0.08  0.11  0.07  0.12]]\n",
      "\n",
      " [[ 0.01  0.01  0.14  0.12  0.03  0.03  0.41  0.26]\n",
      "  [ 0.14  0.27  0.05  0.06  0.15  0.24  0.05  0.03]\n",
      "  [ 0.14  0.14  0.14  0.14  0.12  0.17  0.08  0.07]\n",
      "  [ 0.05  0.19  0.09  0.2   0.05  0.2   0.05  0.17]\n",
      "  [ 0.19  0.17  0.06  0.04  0.24  0.2   0.06  0.04]\n",
      "  [ 0.24  0.11  0.11  0.07  0.19  0.16  0.08  0.03]]\n",
      "\n",
      " [[ 0.08  0.05  0.05  0.02  0.26  0.17  0.24  0.13]\n",
      "  [ 0.06  0.04  0.05  0.03  0.3   0.23  0.16  0.13]\n",
      "  [ 0.21  0.13  0.09  0.05  0.24  0.2   0.05  0.04]\n",
      "  [ 0.01  0.01  0.17  0.12  0.03  0.04  0.34  0.28]\n",
      "  [ 0.11  0.12  0.16  0.12  0.13  0.13  0.11  0.11]\n",
      "  [ 0.04  0.12  0.08  0.23  0.05  0.17  0.08  0.23]]\n",
      "\n",
      " [[ 0.25  0.16  0.09  0.05  0.2   0.14  0.07  0.03]\n",
      "  [ 0.23  0.19  0.07  0.05  0.22  0.18  0.04  0.02]\n",
      "  [ 0.05  0.04  0.02  0.01  0.43  0.33  0.06  0.05]\n",
      "  [ 0.18  0.17  0.06  0.05  0.26  0.22  0.04  0.03]\n",
      "  [ 0.09  0.29  0.05  0.14  0.08  0.24  0.02  0.09]\n",
      "  [ 0.11  0.07  0.24  0.15  0.1   0.09  0.14  0.1 ]]\n",
      "\n",
      " [[ 0.04  0.03  0.08  0.04  0.2   0.15  0.28  0.18]\n",
      "  [ 0.02  0.01  0.13  0.09  0.12  0.1   0.32  0.2 ]\n",
      "  [ 0.12  0.09  0.16  0.09  0.19  0.17  0.12  0.07]\n",
      "  [ 0.01  0.06  0.03  0.1   0.06  0.29  0.08  0.37]\n",
      "  [ 0.06  0.05  0.24  0.26  0.05  0.09  0.11  0.14]\n",
      "  [ 0.01  0.03  0.03  0.06  0.09  0.38  0.1   0.31]]\n",
      "\n",
      " [[ 0.05  0.04  0.06  0.03  0.27  0.2   0.2   0.15]\n",
      "  [ 0.02  0.02  0.34  0.21  0.03  0.02  0.24  0.12]\n",
      "  [ 0.21  0.14  0.1   0.06  0.22  0.18  0.06  0.03]\n",
      "  [ 0.12  0.16  0.08  0.11  0.15  0.24  0.06  0.07]\n",
      "  [ 0.31  0.12  0.06  0.02  0.3   0.12  0.06  0.01]\n",
      "  [ 0.07  0.04  0.03  0.02  0.33  0.37  0.08  0.07]]\n",
      "\n",
      " [[ 0.22  0.11  0.15  0.07  0.15  0.1   0.14  0.05]\n",
      "  [ 0.24  0.2   0.09  0.05  0.22  0.11  0.06  0.03]\n",
      "  [ 0.03  0.06  0.19  0.38  0.03  0.06  0.11  0.15]\n",
      "  [ 0.03  0.03  0.05  0.03  0.24  0.22  0.18  0.23]\n",
      "  [ 0.01  0.05  0.13  0.44  0.01  0.05  0.08  0.23]\n",
      "  [ 0.12  0.1   0.14  0.14  0.12  0.16  0.11  0.1 ]]\n",
      "\n",
      " ..., \n",
      " [[ 0.19  0.12  0.1   0.06  0.24  0.11  0.12  0.06]\n",
      "  [ 0.13  0.28  0.03  0.06  0.17  0.28  0.02  0.03]\n",
      "  [ 0.2   0.21  0.06  0.08  0.18  0.2   0.03  0.04]\n",
      "  [ 0.01  0.06  0.04  0.34  0.02  0.11  0.04  0.39]\n",
      "  [ 0.09  0.25  0.03  0.09  0.11  0.32  0.02  0.08]\n",
      "  [ 0.12  0.12  0.13  0.16  0.13  0.16  0.08  0.1 ]]\n",
      "\n",
      " [[ 0.16  0.1   0.12  0.07  0.17  0.13  0.17  0.08]\n",
      "  [ 0.21  0.18  0.06  0.05  0.24  0.18  0.04  0.04]\n",
      "  [ 0.19  0.14  0.11  0.08  0.2   0.16  0.06  0.05]\n",
      "  [ 0.    0.02  0.02  0.13  0.02  0.09  0.09  0.63]\n",
      "  [ 0.05  0.22  0.02  0.12  0.07  0.36  0.02  0.15]\n",
      "  [ 0.23  0.16  0.09  0.08  0.17  0.18  0.05  0.05]]\n",
      "\n",
      " [[ 0.3   0.16  0.05  0.03  0.22  0.12  0.09  0.03]\n",
      "  [ 0.08  0.05  0.02  0.01  0.41  0.29  0.09  0.05]\n",
      "  [ 0.13  0.23  0.02  0.04  0.16  0.36  0.02  0.03]\n",
      "  [ 0.09  0.17  0.07  0.14  0.1   0.26  0.05  0.11]\n",
      "  [ 0.19  0.17  0.04  0.05  0.19  0.31  0.02  0.04]\n",
      "  [ 0.12  0.1   0.12  0.12  0.15  0.22  0.09  0.1 ]]\n",
      "\n",
      " [[ 0.16  0.12  0.15  0.09  0.12  0.13  0.17  0.06]\n",
      "  [ 0.06  0.05  0.03  0.02  0.35  0.3   0.12  0.06]\n",
      "  [ 0.22  0.16  0.06  0.05  0.23  0.23  0.04  0.02]\n",
      "  [ 0.23  0.17  0.05  0.05  0.21  0.25  0.03  0.03]\n",
      "  [ 0.1   0.22  0.03  0.08  0.11  0.38  0.02  0.08]\n",
      "  [ 0.03  0.07  0.08  0.26  0.03  0.16  0.08  0.29]]\n",
      "\n",
      " [[ 0.02  0.01  0.11  0.07  0.1   0.07  0.39  0.23]\n",
      "  [ 0.01  0.01  0.1   0.08  0.06  0.06  0.4   0.28]\n",
      "  [ 0.01  0.01  0.36  0.22  0.02  0.01  0.24  0.13]\n",
      "  [ 0.1   0.11  0.05  0.05  0.28  0.3   0.06  0.06]\n",
      "  [ 0.07  0.32  0.02  0.1   0.08  0.33  0.01  0.07]\n",
      "  [ 0.    0.    0.02  0.11  0.01  0.03  0.12  0.7 ]]\n",
      "\n",
      " [[ 0.21  0.11  0.15  0.07  0.15  0.11  0.15  0.04]\n",
      "  [ 0.08  0.28  0.03  0.09  0.11  0.33  0.04  0.06]\n",
      "  [ 0.15  0.21  0.06  0.07  0.18  0.25  0.04  0.04]\n",
      "  [ 0.04  0.05  0.02  0.02  0.29  0.43  0.05  0.09]\n",
      "  [ 0.13  0.1   0.13  0.1   0.15  0.19  0.09  0.1 ]\n",
      "  [ 0.17  0.09  0.2   0.08  0.14  0.08  0.2   0.05]]\n",
      "\n",
      " [[ 0.16  0.25  0.06  0.09  0.12  0.19  0.08  0.05]\n",
      "  [ 0.2   0.12  0.14  0.09  0.19  0.13  0.08  0.04]\n",
      "  [ 0.03  0.03  0.03  0.02  0.4   0.28  0.11  0.1 ]\n",
      "  [ 0.05  0.31  0.02  0.09  0.09  0.35  0.01  0.07]\n",
      "  [ 0.08  0.13  0.12  0.18  0.09  0.17  0.07  0.17]\n",
      "  [ 0.01  0.01  0.33  0.25  0.01  0.01  0.22  0.16]]\n",
      "\n",
      " [[ 0.02  0.01  0.15  0.09  0.06  0.04  0.4   0.23]\n",
      "  [ 0.07  0.05  0.08  0.03  0.32  0.21  0.16  0.08]\n",
      "  [ 0.25  0.18  0.07  0.03  0.26  0.13  0.05  0.02]\n",
      "  [ 0.06  0.07  0.07  0.05  0.23  0.27  0.1   0.15]\n",
      "  [ 0.08  0.29  0.04  0.14  0.08  0.27  0.02  0.09]\n",
      "  [ 0.07  0.06  0.14  0.1   0.16  0.16  0.17  0.13]]\n",
      "\n",
      " [[ 0.09  0.22  0.09  0.16  0.06  0.16  0.08  0.14]\n",
      "  [ 0.13  0.16  0.1   0.09  0.2   0.18  0.07  0.08]\n",
      "  [ 0.06  0.06  0.2   0.2   0.09  0.1   0.14  0.15]\n",
      "  [ 0.11  0.07  0.25  0.08  0.15  0.06  0.22  0.07]\n",
      "  [ 0.09  0.1   0.13  0.16  0.13  0.17  0.08  0.13]\n",
      "  [ 0.    0.    0.14  0.1   0.03  0.03  0.38  0.33]]\n",
      "\n",
      " [[ 0.07  0.16  0.15  0.19  0.04  0.09  0.14  0.15]\n",
      "  [ 0.19  0.12  0.21  0.09  0.14  0.09  0.12  0.05]\n",
      "  [ 0.18  0.1   0.22  0.1   0.14  0.09  0.11  0.05]\n",
      "  [ 0.15  0.09  0.21  0.16  0.12  0.1   0.08  0.08]\n",
      "  [ 0.    0.01  0.1   0.44  0.    0.01  0.07  0.36]\n",
      "  [ 0.    0.01  0.07  0.36  0.    0.02  0.08  0.46]]\n",
      "\n",
      " [[ 0.24  0.14  0.08  0.06  0.18  0.18  0.08  0.04]\n",
      "  [ 0.16  0.09  0.16  0.11  0.16  0.15  0.11  0.06]\n",
      "  [ 0.21  0.17  0.04  0.03  0.25  0.23  0.03  0.02]\n",
      "  [ 0.2   0.16  0.04  0.04  0.22  0.27  0.03  0.04]\n",
      "  [ 0.12  0.08  0.18  0.12  0.12  0.14  0.12  0.12]\n",
      "  [ 0.03  0.1   0.08  0.23  0.03  0.16  0.09  0.28]]\n",
      "\n",
      " [[ 0.17  0.29  0.04  0.09  0.08  0.22  0.06  0.04]\n",
      "  [ 0.16  0.09  0.22  0.14  0.11  0.1   0.13  0.05]\n",
      "  [ 0.15  0.14  0.14  0.1   0.15  0.16  0.1   0.05]\n",
      "  [ 0.21  0.18  0.12  0.08  0.17  0.13  0.08  0.04]\n",
      "  [ 0.05  0.04  0.01  0.01  0.4   0.35  0.07  0.06]\n",
      "  [ 0.17  0.12  0.06  0.05  0.24  0.25  0.05  0.05]]\n",
      "\n",
      " [[ 0.17  0.3   0.02  0.07  0.11  0.25  0.03  0.04]\n",
      "  [ 0.25  0.16  0.04  0.03  0.23  0.22  0.04  0.02]\n",
      "  [ 0.09  0.07  0.01  0.01  0.33  0.4   0.04  0.04]\n",
      "  [ 0.09  0.07  0.02  0.02  0.35  0.32  0.07  0.06]\n",
      "  [ 0.27  0.14  0.11  0.06  0.17  0.12  0.09  0.04]\n",
      "  [ 0.08  0.04  0.04  0.03  0.32  0.27  0.14  0.09]]\n",
      "\n",
      " [[ 0.06  0.05  0.16  0.06  0.14  0.08  0.23  0.21]\n",
      "  [ 0.06  0.09  0.06  0.1   0.15  0.24  0.09  0.2 ]\n",
      "  [ 0.05  0.05  0.29  0.19  0.06  0.04  0.22  0.11]\n",
      "  [ 0.06  0.31  0.03  0.12  0.07  0.3   0.02  0.1 ]\n",
      "  [ 0.02  0.05  0.05  0.08  0.11  0.28  0.09  0.32]\n",
      "  [ 0.03  0.02  0.04  0.04  0.24  0.24  0.15  0.22]]\n",
      "\n",
      " [[ 0.19  0.17  0.11  0.07  0.16  0.13  0.11  0.06]\n",
      "  [ 0.06  0.04  0.03  0.01  0.52  0.13  0.17  0.04]\n",
      "  [ 0.18  0.12  0.15  0.05  0.24  0.08  0.13  0.04]\n",
      "  [ 0.19  0.12  0.16  0.06  0.21  0.09  0.12  0.05]\n",
      "  [ 0.16  0.14  0.1   0.08  0.21  0.18  0.06  0.07]\n",
      "  [ 0.07  0.07  0.22  0.21  0.08  0.06  0.15  0.14]]\n",
      "\n",
      " [[ 0.23  0.15  0.09  0.06  0.2   0.14  0.08  0.04]\n",
      "  [ 0.03  0.03  0.28  0.18  0.05  0.03  0.23  0.16]\n",
      "  [ 0.01  0.01  0.13  0.09  0.06  0.04  0.37  0.3 ]\n",
      "  [ 0.13  0.17  0.12  0.16  0.11  0.16  0.06  0.09]\n",
      "  [ 0.14  0.18  0.06  0.08  0.16  0.26  0.04  0.08]\n",
      "  [ 0.    0.03  0.09  0.53  0.    0.02  0.05  0.27]]\n",
      "\n",
      " [[ 0.07  0.07  0.2   0.1   0.1   0.08  0.23  0.14]\n",
      "  [ 0.03  0.02  0.3   0.25  0.03  0.03  0.19  0.15]\n",
      "  [ 0.01  0.03  0.15  0.44  0.01  0.02  0.11  0.25]\n",
      "  [ 0.08  0.2   0.04  0.09  0.12  0.32  0.03  0.12]\n",
      "  [ 0.    0.01  0.08  0.45  0.    0.01  0.07  0.38]\n",
      "  [ 0.17  0.16  0.07  0.07  0.18  0.23  0.05  0.06]]\n",
      "\n",
      " [[ 0.14  0.13  0.14  0.12  0.11  0.08  0.16  0.12]\n",
      "  [ 0.01  0.01  0.35  0.19  0.01  0.01  0.29  0.13]\n",
      "  [ 0.12  0.31  0.03  0.05  0.13  0.31  0.02  0.03]\n",
      "  [ 0.24  0.15  0.06  0.04  0.22  0.22  0.04  0.03]\n",
      "  [ 0.1   0.25  0.02  0.06  0.11  0.38  0.02  0.07]\n",
      "  [ 0.05  0.2   0.04  0.17  0.06  0.25  0.04  0.21]]\n",
      "\n",
      " [[ 0.25  0.11  0.12  0.05  0.18  0.11  0.13  0.04]\n",
      "  [ 0.27  0.12  0.05  0.02  0.38  0.09  0.07  0.01]\n",
      "  [ 0.06  0.04  0.03  0.02  0.4   0.28  0.09  0.08]\n",
      "  [ 0.02  0.01  0.07  0.05  0.14  0.15  0.25  0.31]\n",
      "  [ 0.06  0.33  0.03  0.13  0.05  0.28  0.02  0.1 ]\n",
      "  [ 0.21  0.2   0.06  0.08  0.16  0.21  0.04  0.04]]\n",
      "\n",
      " [[ 0.07  0.05  0.27  0.14  0.05  0.04  0.29  0.09]\n",
      "  [ 0.13  0.09  0.16  0.06  0.2   0.15  0.14  0.06]\n",
      "  [ 0.1   0.08  0.26  0.11  0.11  0.1   0.16  0.08]\n",
      "  [ 0.09  0.38  0.04  0.11  0.07  0.25  0.01  0.05]\n",
      "  [ 0.07  0.08  0.23  0.19  0.06  0.09  0.12  0.15]\n",
      "  [ 0.09  0.33  0.03  0.1   0.07  0.3   0.02  0.06]]]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "test_input  = val_data\n",
    "test_output = val_output\n",
    "predictions = model.predict(test_input).round(2)\n",
    "print(\"\\nAvg Difference: {}\".format(np.abs(test_output - predictions).mean()))\n",
    "print(\"=== Input ===\\n{}\".format(test_input))\n",
    "print(\"\\n=== True Output ===\\n{}\".format(test_output))\n",
    "print(\"\\n=== Prediction ===\\n{}\".format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss is custom and input tensor is introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Input ===\n",
      "[[[4 8 2]\n",
      "  [5 3 6]\n",
      "  [2 1 5]\n",
      "  [0 6 8]\n",
      "  [1 8 0]\n",
      "  [1 5 7]]\n",
      "\n",
      " [[6 3 8]\n",
      "  [3 5 8]\n",
      "  [6 2 5]\n",
      "  [2 5 1]\n",
      "  [7 1 5]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 3 8]\n",
      "  [8 8 8]\n",
      "  [8 7 3]\n",
      "  [6 2 3]\n",
      "  [4 6 0]\n",
      "  [4 7 2]]\n",
      "\n",
      " [[4 8 1]\n",
      "  [1 7 1]\n",
      "  [1 8 7]\n",
      "  [1 2 1]\n",
      "  [7 7 4]\n",
      "  [5 3 5]]\n",
      "\n",
      " [[5 6 6]\n",
      "  [8 0 0]\n",
      "  [2 7 7]\n",
      "  [6 1 3]\n",
      "  [6 5 8]\n",
      "  [1 2 6]]]\n",
      "\n",
      "=== Prediction ===\n",
      "[[[ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "\n",
      " [[ 0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.]]]\n",
      "\n",
      "=== Losses ===\n",
      "[ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "class InputTensorLossFunction:\n",
    "    def __init__(self,\n",
    "            input_shape  : Tuple[int, int],\n",
    "            output_shape : Tuple[int, int],\n",
    "            input_tensor : Tensor,\n",
    "            ) -> None:\n",
    "        self.output_shape = output_shape\n",
    "        self.input_shape  = input_shape\n",
    "        self.input_tensor = input_tensor\n",
    "    \n",
    "    def exponents_of_2_tensor(self, shape: Tuple[int, int]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Return matrix with shape equal to *shape* of the form:\n",
    "        [2, 4, 8, ..., 2**n]\n",
    "        [2, 4, 8, ..., 2**n]\n",
    "        ...\n",
    "        [2, 4, 8, ..., 2**n]\n",
    "        \"\"\"\n",
    "        m = [[2**i for i in range(shape[1])][::-1] for _ in range(shape[0])]\n",
    "        return T.as_tensor(m)\n",
    "    \n",
    "    def make_meshgrid(self, shape: Tuple[int, int]) -> Tensor:\n",
    "        \"\"\"\n",
    "        Create a meshgrid with shape equal to *shape*.\n",
    "        Example for shape (3, 5):\n",
    "            [[0, 1, 2, 3, 4],\n",
    "             [0, 1, 2, 3, 4],\n",
    "             [0, 1, 2, 3, 4]]\n",
    "        \"\"\"\n",
    "        return (T.mgrid[0:shape[0], 0:shape[1]][1])\n",
    "    \n",
    "    def penalize_noncommitment(self, y_pred: Tensor, multiplier: float) -> Tensor:\n",
    "        # return multiplier * (1 - y_pred.max(-1)).sum(-1)\n",
    "        closeness = (1 - 4 * (y_pred - 0.5)**2)\n",
    "        return multiplier * closeness.sum(-1).sum(-1)\n",
    "        \n",
    "    def __call__(self) -> Callable[[Tensor, Tensor], Tensor]:\n",
    "        \"\"\" Return a loss function that is callable by keras. \"\"\"\n",
    "        def custom_loss(y_true: Tensor, y_pred: Tensor) -> Tensor:\n",
    "            exp2 = self.exponents_of_2_tensor(self.input_shape)\n",
    "            true = ((self.input_tensor % 2) * exp2).sum(-1)\n",
    "            pred = (y_pred * self.make_meshgrid(self.output_shape)).sum(-1)\n",
    "            # return ((true - pred)**2).sum(-1) + self.penalize_noncommitment(y_pred, 1)\n",
    "            return self.penalize_noncommitment(y_pred, 1)\n",
    "        return custom_loss\n",
    "    \n",
    "A = T.dtensor3(\"A\")\n",
    "B = T.dtensor3(\"B\")\n",
    "C = T.dtensor3(\"C\")\n",
    "inputs = data[0:5]\n",
    "y_true = output[0:5]\n",
    "y_pred = output[0:5]\n",
    "D = InputTensorLossFunction(data.shape[1:], output.shape[1:], A)()\n",
    "E = D(B, C)\n",
    "F = theano.function([A, B, C], E, on_unused_input=\"ignore\")\n",
    "evaluation = F(inputs, y_true, y_pred).round(2)\n",
    "print(\"=== Input ===\\n{}\".format(inputs))\n",
    "print(\"\\n=== Prediction ===\\n{}\".format(y_pred))\n",
    "print(\"\\n=== Losses ===\\n{}\".format(evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 6, 3)              0         \n",
      "_________________________________________________________________\n",
      "Dropout 1 (Dropout)          (None, 6, 3)              0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 6, 1024)           1585152   \n",
      "_________________________________________________________________\n",
      "Dropout 2 (Dropout)          (None, 6, 1024)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 6, 1024)           4721664   \n",
      "_________________________________________________________________\n",
      "Dropout 3 (Dropout)          (None, 6, 1024)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 6, 1024)           4721664   \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 6, 8)              8200      \n",
      "=================================================================\n",
      "Total params: 11,036,680\n",
      "Trainable params: 11,036,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "input_layer  = Input(name=\"Input\", shape=data.shape[1:])\n",
    "model_layer  = Dropout(name=\"Dropout 1\", rate=1/2)(input_layer)\n",
    "model_layer  = Bidirectional(GRU(name=\"GRU 1\", units=512, return_sequences=True,\n",
    "                    recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "model_layer  = Dropout(name=\"Dropout 2\", rate=1/2)(model_layer)\n",
    "model_layer  = Bidirectional(GRU(name=\"GRU 2\", units=512, return_sequences=True,\n",
    "                   recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "model_layer  = Dropout(name=\"Dropout 3\", rate=1/2)(model_layer)\n",
    "model_layer  = Bidirectional(GRU(name=\"GRU 3\", units=512, return_sequences=True,\n",
    "                   recurrent_dropout=1/2, implementation=2))(model_layer)\n",
    "output_layer = Dense(name=\"Output\", units=output.shape[2], activation=\"softmax\")(model_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "loss  = InputTensorLossFunction(data.shape[1:], output.shape[1:], input_layer)()\n",
    "opt   = keras.optimizers.RMSprop(lr=10**(-9))\n",
    "model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1024 samples, validate on 512 samples\n",
      "Epoch 1/64\n",
      "27s - loss: 1.5917 - acc: 0.3390 - val_loss: 2.3183 - val_acc: 0.1644\n",
      "Epoch 2/64\n",
      "27s - loss: 1.5976 - acc: 0.3390 - val_loss: 2.2241 - val_acc: 0.1999\n",
      "Epoch 3/64\n",
      "27s - loss: 1.5729 - acc: 0.3402 - val_loss: 2.2318 - val_acc: 0.1999\n",
      "Epoch 4/64\n",
      "27s - loss: 1.5828 - acc: 0.3249 - val_loss: 2.3112 - val_acc: 0.1969\n",
      "Epoch 5/64\n",
      "27s - loss: 1.5659 - acc: 0.3377 - val_loss: 2.2830 - val_acc: 0.1895\n",
      "Epoch 6/64\n",
      "27s - loss: 1.5662 - acc: 0.3407 - val_loss: 2.3115 - val_acc: 0.2002\n",
      "Epoch 7/64\n",
      "27s - loss: 1.5694 - acc: 0.3361 - val_loss: 2.2999 - val_acc: 0.1901\n",
      "Epoch 8/64\n",
      "\n",
      "Epoch 00007: reducing learning rate to 0.1.\n",
      "27s - loss: 1.5664 - acc: 0.3433 - val_loss: 2.3484 - val_acc: 0.2041\n",
      "Epoch 9/64\n",
      "27s - loss: 1.5296 - acc: 0.3569 - val_loss: 2.3380 - val_acc: 0.2051\n",
      "Epoch 10/64\n",
      "27s - loss: 1.5241 - acc: 0.3617 - val_loss: 2.3355 - val_acc: 0.2145\n",
      "Epoch 11/64\n",
      "27s - loss: 1.5001 - acc: 0.3672 - val_loss: 2.3390 - val_acc: 0.2093\n",
      "Epoch 12/64\n",
      "27s - loss: 1.5034 - acc: 0.3740 - val_loss: 2.3589 - val_acc: 0.2155\n",
      "Epoch 13/64\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.010000000149011612.\n",
      "27s - loss: 1.5105 - acc: 0.3639 - val_loss: 2.3429 - val_acc: 0.2158\n",
      "Epoch 14/64\n",
      "27s - loss: 1.4983 - acc: 0.3687 - val_loss: 2.3534 - val_acc: 0.2135\n",
      "Epoch 15/64\n",
      "27s - loss: 1.4980 - acc: 0.3717 - val_loss: 2.3480 - val_acc: 0.2142\n",
      "Epoch 16/64\n",
      "27s - loss: 1.4994 - acc: 0.3695 - val_loss: 2.3429 - val_acc: 0.2155\n",
      "Epoch 17/64\n",
      "27s - loss: 1.4848 - acc: 0.3794 - val_loss: 2.3404 - val_acc: 0.2184\n",
      "Epoch 18/64\n",
      "\n",
      "Epoch 00017: reducing learning rate to 0.0009999999776482583.\n",
      "27s - loss: 1.4966 - acc: 0.3698 - val_loss: 2.3513 - val_acc: 0.2161\n",
      "Epoch 19/64\n",
      "27s - loss: 1.5000 - acc: 0.3778 - val_loss: 2.3460 - val_acc: 0.2165\n",
      "Epoch 20/64\n",
      "27s - loss: 1.4895 - acc: 0.3828 - val_loss: 2.3506 - val_acc: 0.2148\n",
      "Epoch 21/64\n",
      "27s - loss: 1.4962 - acc: 0.3693 - val_loss: 2.3501 - val_acc: 0.2181\n",
      "Epoch 22/64\n",
      "27s - loss: 1.5011 - acc: 0.3660 - val_loss: 2.3386 - val_acc: 0.2148\n",
      "Epoch 23/64\n",
      "\n",
      "Epoch 00022: reducing learning rate to 9.999999310821295e-05.\n",
      "27s - loss: 1.4912 - acc: 0.3766 - val_loss: 2.3452 - val_acc: 0.2148\n",
      "Epoch 24/64\n",
      "27s - loss: 1.4940 - acc: 0.3742 - val_loss: 2.3430 - val_acc: 0.2165\n",
      "Epoch 25/64\n",
      "27s - loss: 1.4937 - acc: 0.3737 - val_loss: 2.3512 - val_acc: 0.2113\n",
      "Epoch 26/64\n",
      "27s - loss: 1.4950 - acc: 0.3752 - val_loss: 2.3551 - val_acc: 0.2158\n",
      "Epoch 27/64\n",
      "27s - loss: 1.4983 - acc: 0.3730 - val_loss: 2.3467 - val_acc: 0.2148\n",
      "Epoch 28/64\n",
      "\n",
      "Epoch 00027: reducing learning rate to 9.999999019782991e-06.\n",
      "27s - loss: 1.5183 - acc: 0.3631 - val_loss: 2.3513 - val_acc: 0.2174\n",
      "Epoch 29/64\n",
      "27s - loss: 1.4909 - acc: 0.3709 - val_loss: 2.3588 - val_acc: 0.2155\n",
      "Epoch 30/64\n",
      "27s - loss: 1.5020 - acc: 0.3703 - val_loss: 2.3572 - val_acc: 0.2129\n",
      "Epoch 31/64\n",
      "27s - loss: 1.5005 - acc: 0.3768 - val_loss: 2.3629 - val_acc: 0.2106\n",
      "Epoch 32/64\n",
      "27s - loss: 1.4914 - acc: 0.3745 - val_loss: 2.3504 - val_acc: 0.2178\n",
      "Epoch 33/64\n",
      "\n",
      "Epoch 00032: reducing learning rate to 9.99999883788405e-07.\n",
      "27s - loss: 1.4997 - acc: 0.3719 - val_loss: 2.3538 - val_acc: 0.2132\n",
      "Epoch 34/64\n",
      "27s - loss: 1.4976 - acc: 0.3675 - val_loss: 2.3520 - val_acc: 0.2122\n",
      "Epoch 35/64\n",
      "27s - loss: 1.4993 - acc: 0.3713 - val_loss: 2.3459 - val_acc: 0.2119\n",
      "Epoch 36/64\n",
      "27s - loss: 1.4918 - acc: 0.3737 - val_loss: 2.3445 - val_acc: 0.2139\n",
      "Epoch 37/64\n",
      "27s - loss: 1.4900 - acc: 0.3800 - val_loss: 2.3500 - val_acc: 0.2139\n",
      "Epoch 38/64\n",
      "\n",
      "Epoch 00037: reducing learning rate to 9.99999883788405e-08.\n",
      "27s - loss: 1.4974 - acc: 0.3700 - val_loss: 2.3599 - val_acc: 0.2083\n",
      "Epoch 39/64\n",
      "27s - loss: 1.5015 - acc: 0.3755 - val_loss: 2.3514 - val_acc: 0.2122\n",
      "Epoch 40/64\n",
      "27s - loss: 1.4880 - acc: 0.3763 - val_loss: 2.3533 - val_acc: 0.2152\n",
      "Epoch 41/64\n",
      "27s - loss: 1.4962 - acc: 0.3768 - val_loss: 2.3546 - val_acc: 0.2145\n",
      "Epoch 42/64\n",
      "27s - loss: 1.4965 - acc: 0.3696 - val_loss: 2.3448 - val_acc: 0.2113\n",
      "Epoch 43/64\n",
      "\n",
      "Epoch 00042: reducing learning rate to 9.999998695775504e-09.\n",
      "27s - loss: 1.5112 - acc: 0.3613 - val_loss: 2.3521 - val_acc: 0.2152\n",
      "Epoch 44/64\n",
      "27s - loss: 1.5092 - acc: 0.3617 - val_loss: 2.3634 - val_acc: 0.2152\n",
      "Epoch 45/64\n",
      "27s - loss: 1.4915 - acc: 0.3763 - val_loss: 2.3486 - val_acc: 0.2132\n",
      "Epoch 46/64\n",
      "27s - loss: 1.4893 - acc: 0.3703 - val_loss: 2.3629 - val_acc: 0.2174\n",
      "Epoch 47/64\n",
      "27s - loss: 1.4965 - acc: 0.3740 - val_loss: 2.3452 - val_acc: 0.2152\n",
      "Epoch 48/64\n",
      "\n",
      "Epoch 00047: reducing learning rate to 9.99999905104687e-10.\n",
      "27s - loss: 1.5065 - acc: 0.3727 - val_loss: 2.3518 - val_acc: 0.2129\n",
      "Epoch 49/64\n",
      "27s - loss: 1.4969 - acc: 0.3683 - val_loss: 2.3464 - val_acc: 0.2168\n",
      "Epoch 50/64\n",
      "27s - loss: 1.4819 - acc: 0.3765 - val_loss: 2.3495 - val_acc: 0.2139\n",
      "Epoch 51/64\n",
      "27s - loss: 1.5056 - acc: 0.3680 - val_loss: 2.3466 - val_acc: 0.2145\n",
      "Epoch 52/64\n",
      "27s - loss: 1.4956 - acc: 0.3711 - val_loss: 2.3495 - val_acc: 0.2155\n",
      "Epoch 53/64\n",
      "\n",
      "Epoch 00052: reducing learning rate to 9.999998606957661e-11.\n",
      "27s - loss: 1.4998 - acc: 0.3721 - val_loss: 2.3522 - val_acc: 0.2139\n",
      "Epoch 54/64\n",
      "27s - loss: 1.5026 - acc: 0.3724 - val_loss: 2.3400 - val_acc: 0.2096\n",
      "Epoch 55/64\n",
      "27s - loss: 1.4986 - acc: 0.3753 - val_loss: 2.3487 - val_acc: 0.2161\n",
      "Epoch 56/64\n",
      "27s - loss: 1.4951 - acc: 0.3836 - val_loss: 2.3539 - val_acc: 0.2184\n",
      "Epoch 57/64\n",
      "27s - loss: 1.4870 - acc: 0.3792 - val_loss: 2.3425 - val_acc: 0.2132\n",
      "Epoch 58/64\n",
      "\n",
      "Epoch 00057: reducing learning rate to 9.99999874573554e-12.\n",
      "27s - loss: 1.5095 - acc: 0.3693 - val_loss: 2.3479 - val_acc: 0.2106\n",
      "Epoch 59/64\n",
      "27s - loss: 1.4926 - acc: 0.3786 - val_loss: 2.3507 - val_acc: 0.2148\n",
      "Epoch 60/64\n",
      "27s - loss: 1.5022 - acc: 0.3659 - val_loss: 2.3475 - val_acc: 0.2139\n",
      "Epoch 61/64\n",
      "27s - loss: 1.5066 - acc: 0.3634 - val_loss: 2.3475 - val_acc: 0.2129\n",
      "Epoch 62/64\n",
      "27s - loss: 1.4957 - acc: 0.3683 - val_loss: 2.3414 - val_acc: 0.2122\n",
      "Epoch 63/64\n",
      "\n",
      "Epoch 00062: reducing learning rate to 9.999999092680235e-13.\n",
      "27s - loss: 1.5074 - acc: 0.3664 - val_loss: 2.3518 - val_acc: 0.2122\n",
      "Epoch 64/64\n",
      "27s - loss: 1.4985 - acc: 0.3763 - val_loss: 2.3579 - val_acc: 0.2096\n",
      "CPU times: user 29min 16s, sys: 904 ms, total: 29min 17s\n",
      "Wall time: 29min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################################################################\n",
    "np.random.seed(1010)\n",
    "histories = model.fit(data, output, epochs=64, batch_size=1, verbose=2,\n",
    "                      validation_data=(val_data, val_output),\n",
    "                     callbacks=[keras.callbacks.ReduceLROnPlateau(\n",
    "                         patience=5, verbose=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "plt.plot(histories.history['loss'])\n",
    "plt.plot(histories.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()\n",
    "plt.plot(histories.history['acc'])\n",
    "plt.plot(histories.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "test_input  = val_data\n",
    "test_output = val_output\n",
    "predictions = model.predict(test_input)\n",
    "print(\"\\nAvg Difference: {}\".format(np.abs(test_output - predictions).mean()))\n",
    "print(\"=== Input ===\\n{}\".format(test_input))\n",
    "print(\"\\n=== True Output ===\\n{}\".format(test_output))\n",
    "print(\"\\n=== Prediction ===\\n{}\".format(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
