{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "from IPython.display import display\n",
    "from typing import Iterable, List, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def flatten(\n",
    "        iterable: Iterable\n",
    "        ) -> Iterable:\n",
    "    \"\"\" Return a flattened iterable from a nested iterable.\n",
    "        [[3, [4, 5]], 6, [[[7]]]] -> [3, 4, 5, 6, 7]\n",
    "    \"\"\"\n",
    "    for item in iterable:\n",
    "        if  isinstance(item, Iterable) and not isinstance(item, (str, bytes)):\n",
    "            yield from flatten(item)\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "\n",
    "def parse_file(\n",
    "        filename         : str,\n",
    "        initial_event_id : int      = 0,\n",
    "        ignored_columns  : Sequence = (),\n",
    "        ) -> Iterable[Iterable]:\n",
    "    \"\"\" Parses the lines in the file from 'filename' to a format\n",
    "        appropriate for passing into a pandas DataFrame constructor.\n",
    "    \"\"\"\n",
    "    event_id = initial_event_id\n",
    "    with open(filename) as file:\n",
    "        lines = filter(None, (line.strip() for line in file))\n",
    "        for line in lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                event_id += 1\n",
    "            else:\n",
    "                j_list = json.loads(\"[{0}]\".format(line))\n",
    "                for column in ignored_columns:\n",
    "                    del j_list[column]\n",
    "                j_list.append(event_id)\n",
    "                yield flatten(j_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "clusters_columns = [\n",
    "    \"hit_nr\", \"barcode\", \"volume_id\", \"layer_id\",\n",
    "    \"lx\",     \"ly\",      \"elx\",       \"ely\",    \n",
    "    \"gx\",     \"gy\",      \"gz\",        \"phi\",    \n",
    "    \"theta\",  \"ephi\",    \"etheta\",    \"event_id\",\n",
    "]\n",
    "particles_columns = [\n",
    "    \"barcode\",  \"vertex_x\", \"vertex_y\",\n",
    "    \"vertex_z\", \"momentum\", \"theta\",\n",
    "    \"phi\",      \"charge\",   \"event_id\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction from a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "number = 1\n",
    "base_directory     = \"/inputdata/ACTS/prod_mu10_pt1000_2017_07_29\"\n",
    "clusters_filename  = base_directory + \"/clusters_{0}.csv\".format(number)\n",
    "particles_filename = base_directory + \"/particles_{0}.csv\".format(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "clusters_lines = parse_file(clusters_filename, ignored_columns=[7])\n",
    "clusters_frame = pd.DataFrame(clusters_lines, columns=clusters_columns)\n",
    "clusters_frame.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "particles_lines = parse_file(particles_filename)\n",
    "particles_frame = pd.DataFrame(particles_lines, columns=particles_columns)\n",
    "particles_frame.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "left_frame     = clusters_frame\n",
    "right_frame    = particles_frame[[\"event_id\", \"barcode\", \"momentum\", \"charge\"]]\n",
    "combined_frame = left_frame.merge(right_frame, on=[\"event_id\", \"barcode\"])\n",
    "combined_frame.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "gx    = combined_frame[\"gx\"]\n",
    "gy    = combined_frame[\"gy\"]\n",
    "gz    = combined_frame[\"gz\"]\n",
    "phi   = np.arctan2(gy, gx)\n",
    "r     = np.sqrt(gx**2 + gy**2)\n",
    "frame = combined_frame.assign(phi=phi, r=r, z=gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Eliminate duplicate hits that were caused by imperfections in the detector.\n",
    "frame = frame.sort_values(\"r\")\n",
    "frame = frame.drop_duplicates([\"event_id\", \"barcode\", \"layer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Specify the volume to use. Each volume is a different detector configuration.\n",
    "frame = frame[frame[\"volume_id\"] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Set radiuses to be the same for each layer.\n",
    "for layer_id in frame[\"layer_id\"].unique():\n",
    "    ind = frame[\"layer_id\"] == layer_id\n",
    "    rs  = frame[ind][\"r\"]\n",
    "    med = rs.median()\n",
    "    frame.loc[ind, \"r\"] = med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Put limits on the number of tracks per event. \n",
    "max_tracks = 50\n",
    "min_tracks = 2\n",
    "frames = [f for (_, f) in frame.groupby(\"event_id\", sort=False)]\n",
    "for i, f in enumerate(frames):\n",
    "    barcodes = f[\"barcode\"].unique()\n",
    "    if len(barcodes) < min_tracks:\n",
    "        frames[i] = pd.DataFrame()\n",
    "    if len(barcodes) > max_tracks:\n",
    "        length = np.random.randint(min_tracks, max_tracks + 1)\n",
    "        barcodes = np.random.choice(barcodes, length, replace=False)\n",
    "        f = f[f[\"barcode\"].isin(barcodes)]\n",
    "        frames[i] = f\n",
    "frame = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Clean up the frame a bit.\n",
    "frame = frame[[\"event_id\", \"barcode\", \"phi\", \"r\", \"z\", \"momentum\", \"charge\"]]\n",
    "frame = frame.sort_values([\"event_id\", \"barcode\", \"r\"])\n",
    "frame.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction from multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def extract(\n",
    "        clusters_filename  : str, \n",
    "        particles_filename : str, \n",
    "        min_tracks         : int = 2, \n",
    "        max_tracks         : int = 50,\n",
    "        initial_event_id   : int = 0,\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\" Everything in one function.\n",
    "        Depending on the size of the file, this function could take a long\n",
    "        time. Most of the time is spent parsing the csv files within the\n",
    "        first 4 lines.\n",
    "    \"\"\"\n",
    "    clusters_lines = parse_file(\n",
    "        clusters_filename,\n",
    "        ignored_columns=[7],\n",
    "        initial_event_id=initial_event_id)\n",
    "    clusters_frame = pd.DataFrame(clusters_lines, columns=clusters_columns)\n",
    "    particles_lines = parse_file(\n",
    "        particles_filename, \n",
    "        initial_event_id=initial_event_id)\n",
    "    particles_frame = pd.DataFrame(particles_lines, columns=particles_columns)\n",
    "    left  = clusters_frame\n",
    "    right = particles_frame[[\"event_id\", \"barcode\", \"momentum\", \"charge\"]]\n",
    "    combined_frame = left.merge(right, on=[\"event_id\", \"barcode\"])\n",
    "    gx    = combined_frame[\"gx\"]\n",
    "    gy    = combined_frame[\"gy\"]\n",
    "    gz    = combined_frame[\"gz\"]\n",
    "    phi   = np.arctan2(gy, gx)\n",
    "    r     = np.sqrt(gx**2 + gy**2)\n",
    "    frame = combined_frame.assign(phi=phi, r=r, z=gz)\n",
    "    frame = frame.sort_values(\"r\")\n",
    "    frame = frame.drop_duplicates([\"event_id\", \"barcode\", \"layer_id\"])\n",
    "    frame = frame[frame[\"volume_id\"] == 8]\n",
    "    for layer_id in frame[\"layer_id\"].unique():\n",
    "        ind = frame[\"layer_id\"] == layer_id\n",
    "        rs  = frame[ind][\"r\"]\n",
    "        med = rs.median()\n",
    "        frame.loc[ind, \"r\"] = med\n",
    "    frames = [f for (_, f) in frame.groupby(\"event_id\", sort=False)]\n",
    "    for i, f in enumerate(frames):\n",
    "        barcodes = f[\"barcode\"].unique()\n",
    "        if len(barcodes) < min_tracks:\n",
    "            frames[i] = pd.DataFrame()\n",
    "        if len(barcodes) > max_tracks:\n",
    "            length = np.random.randint(min_tracks, max_tracks + 1)\n",
    "            barcodes = np.random.choice(barcodes, length, replace=False)\n",
    "            f = f[f[\"barcode\"].isin(barcodes)]\n",
    "            frames[i] = f\n",
    "    frame = pd.concat(frames)\n",
    "    frame = frame[[\"event_id\", \"barcode\", \"phi\", \"r\",\n",
    "                   \"z\", \"momentum\", \"charge\"]]\n",
    "    frame = frame.sort_values([\"event_id\", \"barcode\", \"r\"])\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting from file 1\n",
      "Extracting from file 2\n",
      "169874\n",
      "CPU times: user 1min 56s, sys: 716 ms, total: 1min 57s\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################################################################################\n",
    "frames = []\n",
    "initial_event_id = 0\n",
    "for i in range(1, 1 + 100):\n",
    "    print(\"Extracting from file {0}\".format(i))\n",
    "    try:\n",
    "        base_directory     = \"/inputdata/ACTS/prod_mu10_pt1000_2017_07_29\"\n",
    "        clusters_filename  = base_directory + \"/clusters_{0}.csv\".format(i)\n",
    "        particles_filename = base_directory + \"/particles_{0}.csv\".format(i)\n",
    "        frame = extract(\n",
    "            clusters_filename=clusters_filename, \n",
    "            particles_filename=particles_filename, \n",
    "            min_tracks=2, \n",
    "            max_tracks=50, \n",
    "            initial_event_id=initial_event_id)\n",
    "        initial_event_id = frame[\"event_id\"].max() + 1\n",
    "        frames.append(frame)\n",
    "    except FileNotFoundError as error:\n",
    "        print(error)\n",
    "frame = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Hits: 169874\n",
      "Number of Events: 1983\n",
      "Min Number of Tracks: 2\n",
      "Max Number of Tracks: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Hits: {}\".format(len(frame)))\n",
    "print(\"Number of Events: {}\".format(len(frame[\"event_id\"].unique())))\n",
    "tracks  = [value for (_, value) in frame.groupby([\"event_id\"])]\n",
    "lengths = [len(value[\"barcode\"].unique()) for value in tracks]\n",
    "print(\"Min Number of Tracks: {}\".format(min(lengths)))\n",
    "print(\"Max Number of Tracks: {}\".format(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3954909 bytes\n"
     ]
    }
   ],
   "source": [
    "filepath = \"data/sets/ACTS-MOMENTUMS.gz\"\n",
    "frame.to_csv(filepath, compression=\"gzip\")\n",
    "print(\"{0} bytes\".format(os.path.getsize(filepath)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
