{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct  6 18:12:15 2017       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 1080    Off  | 0000:04:00.0     Off |                  N/A |\n",
      "| 27%   31C    P8     9W / 180W |      0MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 1080    Off  | 0000:05:00.0     Off |                  N/A |\n",
      "| 27%   32C    P8     9W / 180W |      0MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 1080    Off  | 0000:06:00.0     Off |                  N/A |\n",
      "| 27%   30C    P8     9W / 180W |      0MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 1080    Off  | 0000:07:00.0     Off |                  N/A |\n",
      "| 27%   30C    P8     9W / 180W |      0MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 1080    Off  | 0000:0B:00.0     Off |                  N/A |\n",
      "| 27%   31C    P8    10W / 180W |      0MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 1080    Off  | 0000:0C:00.0     Off |                  N/A |\n",
      "| 27%   35C    P8    10W / 180W |      0MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 1080    Off  | 0000:0D:00.0     Off |                  N/A |\n",
      "| 27%   32C    P8     9W / 180W |      0MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 1080    Off  | 0000:0E:00.0     Off |                  N/A |\n",
      "| 27%   31C    P8    10W / 180W |    313MiB /  8114MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID  Type  Process name                               Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "from IPython.display import display\n",
    "from typing import Iterable, List, Sequence\n",
    "\n",
    "pd.options.display.max_columns = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def flatten(\n",
    "        iterable: Iterable\n",
    "        ) -> Iterable:\n",
    "    \"\"\"\n",
    "    Return a flattened iterable from a nested iterable.\n",
    "    [[3, [4, 5]], 6, [[[7]]]] -> [3, 4, 5, 6, 7]\n",
    "    \n",
    "    Arguments:\n",
    "        iterable\n",
    "            Some Iterable object that may or may not contain more Iterable\n",
    "            objects.\n",
    "            \n",
    "    Yields the elements from each Iterable or single element from iterable.\n",
    "    \"\"\"\n",
    "    for item in iterable:\n",
    "        if  isinstance(item, Iterable) and not isinstance(item, (str, bytes)):\n",
    "            yield from flatten(item)\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "\n",
    "def parse_file(\n",
    "        filename         : str,\n",
    "        initial_event_id : int      = 0,\n",
    "        ignored_columns  : Sequence = (),\n",
    "        ) -> Iterable[Iterable]:\n",
    "    \"\"\" \n",
    "    Parses the lines in the file from 'filename' to a format\n",
    "    appropriate for passing into a pandas DataFrame constructor.\n",
    "        \n",
    "    Arguments:\n",
    "        filename\n",
    "            The name of the file to parse.\n",
    "        initial_event_id\n",
    "            The event ID that the first event extracted has.\n",
    "            Event ID is incremented by 1 after finishing the\n",
    "            parsing of an event.\n",
    "        ignored_columns\n",
    "            The list of indices of the columns to delete from each line.\n",
    "    \n",
    "    For each line, yields a generator that yield the elements from the line.\n",
    "    \"\"\"\n",
    "    event_id = initial_event_id\n",
    "    with open(filename) as file:\n",
    "        lines = filter(None, (line.strip() for line in file))\n",
    "        for line in lines:\n",
    "            if line.startswith(\"#\"):\n",
    "                event_id += 1\n",
    "            else:\n",
    "                j_list = json.loads(\"[{0}]\".format(line))\n",
    "                for column in ignored_columns:\n",
    "                    del j_list[column]\n",
    "                j_list.append(event_id)\n",
    "                yield flatten(j_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "clusters_columns = [\n",
    "    \"hit_nr\", \"barcode\", \"volume_id\", \"layer_id\",\n",
    "    \"lx\",     \"ly\",      \"elx\",       \"ely\",    \n",
    "    \"gx\",     \"gy\",      \"gz\",        \"phi\",    \n",
    "    \"theta\",  \"ephi\",    \"etheta\",    \"event_id\",\n",
    "]\n",
    "particles_columns = [\n",
    "    \"barcode\",  \"vertex_x\", \"vertex_y\",\n",
    "    \"vertex_z\", \"momentum\", \"theta\",\n",
    "    \"phi\",      \"charge\",   \"event_id\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction from a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "number = 1\n",
    "base_directory     = \"/inputdata/ACTS/prod_mu10_pt1000_2017_07_29\"\n",
    "clusters_filename  = base_directory + \"/clusters_{0}.csv\".format(number)\n",
    "particles_filename = base_directory + \"/particles_{0}.csv\".format(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Example of how the file looks.\n",
    "with open(clusters_filename, \"r\") as file:\n",
    "    for _ in range(5):\n",
    "        print(file.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Quick note: I am ignoring the 7th column within each line.\n",
    "# This column contains [[fch0, fch1, fchdata]]. \n",
    "clusters_lines = parse_file(clusters_filename, ignored_columns=[7])\n",
    "clusters_frame = pd.DataFrame(clusters_lines, columns=clusters_columns)\n",
    "clusters_frame.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "clusters_frame = clusters_frame.rename(columns={\n",
    "    \"hit_nr\": \"hit_number\", \"barcode\": \"cluster_id\", \"lx\": \"local_x\",\n",
    "    \"ly\": \"local_y\", \"gx\": \"x\", \"gy\": \"y\", \"gz\": \"z\", \"elx\": \"local_x_error\",\n",
    "    \"ely\": \"local_y_error\", \"ephi\": \"phi_error\", \"etheta\": \"theta_error\"\n",
    "})\n",
    "clusters_frame.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "events = clusters_frame.groupby([\"event_id\"])[\"cluster_id\"]\n",
    "tracks_per_event = [len(event.unique()) for (_, event) in events]\n",
    "\n",
    "print(\"Hits:\", len(clusters_frame))\n",
    "print(\"Events:\", len(clusters_frame[\"event_id\"].unique()))\n",
    "print(\"Min Tracks:\", min(tracks_per_event))\n",
    "print(\"Max Tracks:\", max(tracks_per_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "particles_lines = parse_file(particles_filename)\n",
    "particles_frame = pd.DataFrame(particles_lines, columns=particles_columns)\n",
    "particles_frame.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "particles_frame = particles_frame.rename(columns={\n",
    "    \"barcode\": \"cluster_id\", \"theta\": \"momentum_theta\", \"phi\": \"momentum_phi\"\n",
    "})\n",
    "particles_frame.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "events = particles_frame.groupby([\"event_id\"])[\"cluster_id\"]\n",
    "tracks_per_event = [len(event.unique()) for (_, event) in events]\n",
    "\n",
    "print(\"Hits:\", len(particles_frame))\n",
    "print(\"Events:\", len(particles_frame[\"event_id\"].unique()))\n",
    "print(\"Min Tracks:\", min(tracks_per_event))\n",
    "print(\"Max Tracks:\", max(tracks_per_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "combined_frame = clusters_frame.merge(\n",
    "    particles_frame,\n",
    "    on=[\"event_id\", \"cluster_id\"])\n",
    "combined_frame.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "x    = combined_frame[\"x\"]\n",
    "y    = combined_frame[\"y\"]\n",
    "frame = combined_frame.assign(r=np.sqrt(x**2 + y**2))\n",
    "frame.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Eliminate duplicate hits that were caused by imperfections in the detector.\n",
    "frame = frame.sort_values(\"r\")\n",
    "frame = frame.drop_duplicates([\"event_id\", \"cluster_id\", \"layer_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Specify the volume to use. Each volume is a different detector configuration.\n",
    "frame = frame[frame[\"volume_id\"] == 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Set radiuses to be the same for each layer.\n",
    "for layer_id in frame[\"layer_id\"].unique():\n",
    "    ind = frame[\"layer_id\"] == layer_id\n",
    "    rs  = frame[ind][\"r\"]\n",
    "    med = rs.median()\n",
    "    frame.loc[ind, \"r\"] = med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Clean up the frame a bit.\n",
    "frame = frame.sort_values([\"event_id\", \"cluster_id\", \"r\"])\n",
    "frame.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "events = frame.groupby([\"event_id\"])[\"cluster_id\"]\n",
    "tracks_per_event = [len(event.unique()) for (_, event) in events]\n",
    "\n",
    "print(\"Hits:\", len(frame))\n",
    "print(\"Events:\", len(frame[\"event_id\"].unique()))\n",
    "print(\"Min Tracks:\", min(tracks_per_event))\n",
    "print(\"Max Tracks:\", max(tracks_per_event))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction from multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "def get_clusters_frame(\n",
    "        clusters_filename : str,\n",
    "        initial_event_id  : int,\n",
    "        ) -> pd.DataFrame:\n",
    "    clusters_lines = parse_file(\n",
    "        clusters_filename,\n",
    "        ignored_columns=[7],\n",
    "        initial_event_id=initial_event_id)\n",
    "    clusters_frame = pd.DataFrame(clusters_lines, columns=clusters_columns)\n",
    "    clusters_frame = clusters_frame.rename(columns={\n",
    "        \"hit_nr\": \"hit_number\", \"barcode\": \"cluster_id\", \"lx\": \"local_x\",\n",
    "        \"ly\": \"local_y\", \"gx\": \"x\", \"gy\": \"y\", \"gz\": \"z\", \"elx\":\n",
    "        \"local_x_error\", \"ely\": \"local_y_error\", \"ephi\": \"phi_error\",\n",
    "        \"etheta\": \"theta_error\"})\n",
    "    return clusters_frame\n",
    "\n",
    "def get_particles_frame(\n",
    "        particles_filename : str,\n",
    "        initial_event_id   : int,\n",
    "        ) -> pd.DataFrame:\n",
    "    particles_lines = parse_file(\n",
    "        particles_filename, \n",
    "        initial_event_id=initial_event_id)\n",
    "    particles_frame = pd.DataFrame(particles_lines, columns=particles_columns)\n",
    "    particles_frame = particles_frame.rename(columns={\n",
    "        \"barcode\": \"cluster_id\", \"theta\": \"momentum_theta\",\n",
    "        \"phi\": \"momentum_phi\"})\n",
    "    return particles_frame\n",
    "\n",
    "def extract(\n",
    "        clusters_filename  : str, \n",
    "        particles_filename : str,\n",
    "        initial_event_id   : int = 0,\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\" Everything in one function.\n",
    "        Depending on the size of the file, this function could take a long\n",
    "        time. Most of the time is spent parsing the csv files within the\n",
    "        first 4 lines.\n",
    "    \"\"\"\n",
    "    clusters  = get_clusters_frame(clusters_filename, initial_event_id)\n",
    "    particles = get_particles_frame(particles_filename, initial_event_id)\n",
    "    combined  = clusters.merge(particles, on=[\"event_id\", \"cluster_id\"])\n",
    "    volume    = combined[combined[\"volume_id\"] == 8]\n",
    "    \n",
    "    return (\n",
    "        volume.assign(r=np.sqrt(combined[\"x\"]**2 + combined[\"y\"]**2))\n",
    "              .sort_values([\"event_id\", \"cluster_id\", \"r\"])\n",
    "              .drop_duplicates([\"event_id\", \"cluster_id\", \"layer_id\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "################################################################################\n",
    "frames = []\n",
    "initial_event_id = 0\n",
    "base_directory = \"/inputdata/ACTS/prod_mu10_pt1000_2017_07_29\"\n",
    "for i in range(1, 1 + 100):\n",
    "    print(\"Extracting from file {0}. Initial Event ID is {1}\".format(i, initial_event_id))\n",
    "    try:\n",
    "        clusters_filename  = base_directory + \"/clusters_{0}.csv\".format(i)\n",
    "        particles_filename = base_directory + \"/particles_{0}.csv\".format(i)\n",
    "        frame = extract(\n",
    "            clusters_filename=clusters_filename, \n",
    "            particles_filename=particles_filename,\n",
    "            initial_event_id=initial_event_id,)\n",
    "        initial_event_id = frame[\"event_id\"].max() + 1\n",
    "        frames.append(frame)\n",
    "    except FileNotFoundError as error:\n",
    "        print(error)\n",
    "print(\"All done. Concatenating frames.\")\n",
    "frame = pd.concat(frames)\n",
    "for layer_id in frame[\"layer_id\"].unique():\n",
    "    ind = frame[\"layer_id\"] == layer_id\n",
    "    rs  = frame[ind][\"r\"]\n",
    "    med = rs.median()\n",
    "    frame.loc[ind, \"r\"] = med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of Hits: {}\".format(len(frame)))\n",
    "print(\"Number of Events: {}\".format(len(frame[\"event_id\"].unique())))\n",
    "tracks  = [value for (_, value) in frame.groupby([\"event_id\"])]\n",
    "lengths = [len(value[\"cluster_id\"].unique()) for value in tracks]\n",
    "print(\"Min Number of Tracks: {}\".format(min(lengths)))\n",
    "print(\"Max Number of Tracks: {}\".format(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.to_csv(\"data/sets/ACTS-MU10-PT1000-COMPLETE.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = pd.read_csv(\"data/sets/ACTS-MU10-PT1000-COMPLETE.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tracker.extractor' from '/home/jovyan/work/Fermi-Internship-2017/Track/tracker/extractor.py'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tracker import extractor\n",
    "importlib.reload(extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = frame[frame[\"event_id\"] < 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared = extractor.prepare_frame(frame)\n",
    "prepared = prepared.sort_values([\"event_id\", \"cluster_id\", \"r\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepared.to_csv(\"data/sets/ACTS-MU10-PT1000-PREPARED.gz\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
