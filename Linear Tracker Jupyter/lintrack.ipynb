{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a program that will train a model to identify and assign hits to tracks.\n",
    "Written by Daniel Zurawski & Keshav Kapoor for Fermilab Summer 2017 internship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T20:02:06.220738Z",
     "start_time": "2017-06-27T20:00:40.590747Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import random\n",
    "import winsound\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D as ax\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is borrowed from a DS&HEP tutorial.\n",
    "It is used to graph the histories after fitting a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T20:02:06.437581Z",
     "start_time": "2017-06-27T20:02:06.226550Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_losses( histories ):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    #plt.ylim(bottom=0)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Error by Epoch')\n",
    "    colors=[]\n",
    "    do_acc=False\n",
    "    for label,loss in histories:\n",
    "        color = tuple([0.1, 0.1, 0.1])\n",
    "        colors.append(color)\n",
    "        l = label\n",
    "        vl= label+\" validation\"\n",
    "        if 'acc' in loss.history:\n",
    "            l+=' (acc %2.4f)'% (loss.history['acc'][-1])\n",
    "            do_acc = True\n",
    "        if 'val_acc' in loss.history:\n",
    "            vl+=' (acc %2.4f)'% (loss.history['val_acc'][-1])\n",
    "            do_acc = True\n",
    "        plt.plot(loss.history['loss'], label=l, color=color)\n",
    "        if 'val_loss' in loss.history:\n",
    "            plt.plot(loss.history['val_loss'], lw=2, ls='dashed', label=vl, color=color)\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    if not do_acc: return\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    for i,(label,loss) in enumerate(histories):\n",
    "        color = tuple([0.0, 0.0, 1.0])\n",
    "        if 'acc' in loss.history:\n",
    "            plt.plot(loss.history['acc'], lw=2, label=label+\" accuracy\", color=color)\n",
    "        if 'val_acc' in loss.history:\n",
    "            plt.plot(loss.history['val_acc'], lw=2, ls='dashed', label=label+\" validation accuracy\", color=color)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T20:02:06.498151Z",
     "start_time": "2017-06-27T20:02:06.445407Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def row_max(modelPredictions):\n",
    "    discreteOut = np.zeros(modelPredictions.size)\n",
    "    maxInd = np.argmax(modelPredictions)\n",
    "    discreteOut[maxInd] = 1\n",
    "    return discreteOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a LinearTracker class.\n",
    "This class is used to load input and output from a .csv file in the correct format for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T20:14:38.617579Z",
     "start_time": "2017-06-27T20:14:34.450268Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearTracker():\n",
    "    \"\"\" An object that classifies particles to tracks after an event. \"\"\"    \n",
    "    def __init__(self, dataframe, model=None):\n",
    "        \"\"\" Initialize a LinearTracker.\n",
    "            @param dataframe - pd.DataFrame - used to pick tracks from.\n",
    "                The headers should contain: (\"id\", \"act_z\", \"r\", \"phi\").\n",
    "            @param model - keras model - A network model that the tracker will\n",
    "                use to classify particles.\n",
    "            @return Nothing\n",
    "        \"\"\"\n",
    "        self.model     = model     # keras model to figure out tracks.\n",
    "        self.dataframe = dataframe # pandas.DataFrame for picking tracks.\n",
    "        self.input     = None      # input to train model on.\n",
    "        self.output    = None      # output to train model on.\n",
    "    # END function __init__\n",
    "    \n",
    "    def load_ramp_data(self, num_events, tracks_per_event, track_size, noise_per_event):\n",
    "        hits_per_event = (track_size * tracks_per_event) + noise_per_event\n",
    "        labels = [\"iphi\", \"layer\", \"act_z\"]\n",
    "        events = self.dataframe[[\"event_id\", \"cluster_id\", \"layer\", \"iphi\", \"act_z\"]].groupby(\"event_id\")\n",
    "        goods = []\n",
    "        bads  = []\n",
    "        for (event_id, event) in list(events):\n",
    "            event.sort_values([\"cluster_id\", \"layer\"], inplace=True)\n",
    "            clusterInd = event.columns.get_loc(\"cluster_id\")\n",
    "            if((event[\"cluster_id\"].max(axis=0)+1) == tracks_per_event):\n",
    "                hits = event[event[\"cluster_id\"] < track_size] \n",
    "#            elif((event[\"cluster_id\"].max(axis=0)+1) >= tracks_per_event):\n",
    "#                goodTracks = pd.DataFrame(columns=[\"cluster_id\", \"layer\", \"iphi\", \"act_z\"])\n",
    "#                for hit in event[1:]:\n",
    "#                    #print(hit)\n",
    "#                    if hit[clusterInd] < tracks_per_event:\n",
    "#                        goodTracks.append(hit)\n",
    "#                goods.append(goodTracks)\n",
    "            elif((event[\"cluster_id\"].max(axis=0)+1) <= tracks_per_event):\n",
    "                bads.append(event)\n",
    "        #print(goods)\n",
    "        self.input  = np.zeros((num_events, hits_per_event, len(labels)))\n",
    "        self.output = np.zeros((num_events, hits_per_event, tracks_per_event + 1))\n",
    "        for n in range(num_events):\n",
    "            #Retrieve the tracks from an event.\n",
    "            tracks = goods\n",
    "            \n",
    "            #Make a mapping from track ID to index within a matrix.\n",
    "            #T2I = self.__get_matrix_map__(tracks)\n",
    "            \n",
    "            #noise  = bads[n].sample(noise_per_event)\n",
    "            #hits   = pd.concat([tracks[n]] + [noise]).sort_values(labels)\n",
    "            \n",
    "            self.input[n] = tracks[n][[\"cluster_id\", \"iphi\", \"layer\"]].values\n",
    "            self.output[n] = keras.utils.to_categorical(tracks[n][\"cluster_id\"].values, (tracks_per_event + noise_per_event))\n",
    "        print(self.input)\n",
    "    def load_data(self, num_events, tracks_per_event, track_size, noise_per_event):\n",
    "        \"\"\" Load input and output data from this object's dataframe.\n",
    "            @param num_events - int - The number of events to generate.\n",
    "            @param tracks_per_event - int - The number of tracks per event.\n",
    "            @param track_size - int - The number of hits per track.\n",
    "            @param noise_per_event - int - The number of hits with no track.\n",
    "            @return Nothing\n",
    "                However, self.input and self.output become numpy arrays.\n",
    "                self.input is collection of hits of shape:\n",
    "                    (num_events, hits_per_event, 3)\n",
    "                self.output is list of probability matrices of shape:\n",
    "                    (num_events, hits_per_event, tracks_per_event)\n",
    "        \"\"\"\n",
    "        hits_per_event = (track_size * tracks_per_event) + noise_per_event\n",
    "        labels = [\"iphi\", \"layer\", \"act_z\"]\n",
    "        groups = self.dataframe[[\"cluster_id\", \"layer\", \"iphi\", \"act_z\"]].groupby(\"custer_id\")\n",
    "        goods  = groups.filter(lambda track: len(track) == track_size)\n",
    "        bads   = groups.filter(lambda track: len(track) != track_size)\n",
    "        \n",
    "        # Populate input and output with data.\n",
    "        goods_group = [g[1] for g in list(goods.groupby(\"cluster_id\"))]\n",
    "        self.input  = np.zeros((num_events, hits_per_event, len(labels)))\n",
    "        self.output = np.zeros((num_events, hits_per_event, tracks_per_event + 1))\n",
    "        for n in range(num_events):\n",
    "            # Retrieve a sample of tracks.\n",
    "            tracks = random.sample(goods_group, tracks_per_event)\n",
    "            \n",
    "            # Make a mapping from track ID to index within a matrix.\n",
    "            T2I = self.__get_matrix_map__(tracks) # Track to Index\n",
    "            \n",
    "            # Make some noise hits to add.\n",
    "            noise  = bads.sample(noise_per_event)\n",
    "            hits   = pd.concat(tracks + [noise]).sort_values(labels)\n",
    "            \n",
    "            self.__populate_input__(hits, labels, n)\n",
    "            self.__populate_output__(hits, T2I, n, tracks_per_event)\n",
    "    # END FUNCTION load_data\n",
    "    \n",
    "    def plot(self, event_index=None, in_data=None, out_data=None):\n",
    "        \"\"\" Display a 3D plot of the event with event index 'eventID'.\n",
    "        \"\"\"\n",
    "        if event_index is not None:\n",
    "            in_data  = self.input[event_index]\n",
    "            out_data = self.output[event_index]\n",
    "        elif in_data is None or out_data is None:\n",
    "            print(\"Please provide an event_index argument.\")\n",
    "            return\n",
    "        \n",
    "        # Convert the input (phi, r, z) to cartesian (x, y, z)\n",
    "        conv = lambda PRZ : (np.cos(PRZ[0])*PRZ[1],np.sin(PRZ[0])*PRZ[1],PRZ[2])\n",
    "        XYZ  = np.array([conv(hit) for hit in in_data])\n",
    "        \n",
    "        # Get the colors.\n",
    "        cmap = plt.cm.get_cmap('hsv', out_data.shape[1])\n",
    "        tracks, colors = self.__trackify__(XYZ, out_data, cmap)\n",
    "\n",
    "        # Create the plot.\n",
    "        plot = plt.figure().add_subplot(111, projection='3d')        \n",
    "        for i, track in enumerate(tracks[:-1]):\n",
    "            plot.plot(xs=[t[0] for t in track],\n",
    "                      ys=[t[1] for t in track],\n",
    "                      zs=[t[2] for t in track],\n",
    "                      c=colors[i], linestyle='-', marker='o')\n",
    "        plot.scatter(xs=[t[0] for t in tracks[-1]],\n",
    "                     ys=[t[1] for t in tracks[-1]],\n",
    "                     zs=[t[2] for t in tracks[-1]],\n",
    "                     c=colors[-1])\n",
    "        plt.show(plot)\n",
    "    # END FUNCTION create_plot\n",
    "    \n",
    "    def __trackify__(self, in_data, out_data, cmap):\n",
    "        \"\"\"\n",
    "            @return a pair such that:\n",
    "                pair[0] - list of tracks, where a track is a list of hits.\n",
    "                pair[1] - list of colors with index corresponding to how\n",
    "                    to color the track at that index.\n",
    "        \"\"\"\n",
    "        indices = [np.argmax(out_data[i]) for i in range(in_data.shape[0])]\n",
    "        tracks  = [[] for _ in range(out_data.shape[1])]\n",
    "        for i, hit in enumerate(in_data):\n",
    "            tracks[indices[i]].append(hit)\n",
    "        tracks = np.array(tracks)\n",
    "        colors = np.array([cmap(i) for i in range(len(indices))])\n",
    "        return (tracks, colors)\n",
    "    # END FUNCTION __trackify__\n",
    "    \n",
    "    def show_losses(self, histories):\n",
    "        \"\"\" Graph the accuracy and loss of a model's histories.\n",
    "            Code from HEPTrks keras tutorial file. in DSHEP folder.\n",
    "            @param histories - list of pairs (string, history from model) \n",
    "            @return Nothing\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.ylim(bottom=0)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Error by Epoch')\n",
    "        colors=[]\n",
    "        do_acc=False\n",
    "        for label,loss in histories:\n",
    "            color = tuple([0.1, 0.1, 0.1])\n",
    "            colors.append(color)\n",
    "            l = label\n",
    "            vl= label+\" validation\"\n",
    "            if 'acc' in loss.history:\n",
    "                l+=' (acc %2.4f)'% (loss.history['acc'][-1])\n",
    "                do_acc = True\n",
    "            if 'val_acc' in loss.history:\n",
    "                vl+=' (acc %2.4f)'% (loss.history['val_acc'][-1])\n",
    "                do_acc = True\n",
    "            plt.plot(loss.history['loss'], label=l, color=color)\n",
    "            if 'val_loss' in loss.history:\n",
    "                plt.plot(loss.history['val_loss'], lw=2, ls='dashed', label=vl, color=color)\n",
    "        plt.legend()\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "        if not do_acc: return\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        for i,(label,loss) in enumerate(histories):\n",
    "            color = tuple([0.0, 0.0, 1.0])\n",
    "            if 'acc' in loss.history:\n",
    "                plt.plot(loss.history['acc'], lw=2, label=label+\" accuracy\", color=color)\n",
    "            if 'val_acc' in loss.history:\n",
    "                plt.plot(loss.history['val_acc'], lw=2, ls='dashed', label=label+\" validation accuracy\", color=color)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "    # END FUNCTION show_losses\n",
    "       \n",
    "    def __populate_input__(self, hits, labels, event_index):\n",
    "        \"\"\" Populate the input at event with index 'event_index'.\n",
    "            @param hits - pd.DataFrame\n",
    "                The pd.DataFrame of hits to set this event to.\n",
    "            @param labels - The categories we want from the hits pd.DataFrame.\n",
    "            @param event_index - int\n",
    "                Index of the event to set.\n",
    "            @return Nothing\n",
    "        \"\"\"\n",
    "        self.input[event_index, :] = hits[labels].values\n",
    "    # END FUNCTION __populate_input__\n",
    "    \n",
    "    def __populate_output__(self, hits, mapping, event_index, tracks_per_event):\n",
    "        \"\"\" Populate the output at event with index 'event_index'.\n",
    "            @param hits - pd.DataFrame\n",
    "                The pd.DataFrame of hits to set this event to.\n",
    "            @param mapping - dictionary object (int -> int)\n",
    "                A dictionary object that maps track ID to matrix index.\n",
    "            @param event_index - int\n",
    "                Index of the event to set.\n",
    "            @param tracks_per_event - int\n",
    "                The number of tracks per event.\n",
    "                The last column (index: tracks_per_event) is the noise column.\n",
    "            @return Nothing\n",
    "        \"\"\"\n",
    "        noise_index = tracks_per_event\n",
    "        for t, track_ID in enumerate(hits[\"cluster_id\"]):\n",
    "            index = mapping.get(track_ID)\n",
    "            if index is not None:\n",
    "                self.output[event_index, t, index] = 1\n",
    "            else:\n",
    "                self.output[event_index, t, noise_index] = 1\n",
    "    # END FUNCTION __populate_output__\n",
    "    \n",
    "    def __get_matrix_map__(self, tracks):\n",
    "        \"\"\" Get a dictionary that maps track ID to matrix index.\n",
    "            @param tracks - list of pd.DataFrames\n",
    "                Each pd.DataFrame consists of its hits.\n",
    "            @return dictionary object (int -> int)\n",
    "                A mapping from track ID to matrix index.\n",
    "        \"\"\"\n",
    "        L = pd.concat([T.sort_values([\"layer\"]).head(1) for T in tracks])\n",
    "        L.sort_values([\"iphi\", \"act_z\"], inplace=True)\n",
    "        return dict((hit, idx) for idx, hit in enumerate(L[\"cluster_id\"]))\n",
    "    # END FUNCTION __get_matrix_map__\n",
    "# END CLASS LinearTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is how to create a LinearTracker and how to load data into it. It is important to note that after construction, a LinearTracker must call its load_data() function with user specifications for how data should be loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a ValueError describing how the population is not large enough for the sample, then that means that the data\n",
    "loaded in from the .csv file does not contain enough tracks of size 'track_size'. Try to either load in a larger\n",
    "population or change the 'track_size' variable to a different positive integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T20:14:41.120141Z",
     "start_time": "2017-06-27T20:14:39.611703Z"
    }
   },
   "outputs": [],
   "source": [
    "filename  = ('../Data Sets/corrected_public_train.csv')\n",
    "dataframe = pd.read_csv(filename)\n",
    "tracker   = LinearTracker(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into the input and output member variables of LinearTracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T20:38:52.505210Z",
     "start_time": "2017-06-27T20:27:54.895671Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "tracker.load_ramp_data(num_events=8, tracks_per_event=5, track_size=4, noise_per_event=0)\n",
    "print(\"Ding! All done.\")\n",
    "winsound.Beep(2200, 1000)\n",
    "winsound.Beep(1800, 1000)\n",
    "winsound.Beep(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the input and output training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T19:30:25.596528Z",
     "start_time": "2017-06-27T19:30:25.127688Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display,HTML\n",
    "\n",
    "def multi_column_df_display(list_dfs, cols=2):\n",
    "    \"\"\" Code by David Medenjak responding to StackOverflow question found here:\n",
    "        https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side\n",
    "        Displays a list of dataframes in IPython as a table with cols number of columns.\n",
    "    \"\"\"\n",
    "    html_table = \"<table style='width:100%; border:0px'>{content}</table>\"\n",
    "    html_row = \"<tr style='border:0px'>{content}</tr>\"\n",
    "    html_cell = \"<td style='width:{width}%;vertical-align:top;border:0px'>{{content}}</td>\"\n",
    "    html_cell = html_cell.format(width=100/cols)\n",
    "\n",
    "    cells = [ html_cell.format(content=df.to_html()) for df in list_dfs ]\n",
    "    cells += (cols - (len(list_dfs)%cols)) * [html_cell.format(content=\"\")] # pad\n",
    "    rows = [ html_row.format(content=\"\".join(cells[i:i+cols])) for i in range(0,len(cells),cols)]\n",
    "    display(HTML(html_table.format(content=\"\".join(rows))))\n",
    "# END FUNCTION multi_column_df_display\n",
    "\n",
    "input_cols  = [\"iphi\", \"layer\", \"act_z\"]\n",
    "output_cols = [\"T{}\".format(i) for i in range(tracker.output.shape[2] - 1)] + [\"N\"]\n",
    "show_max    = 2\n",
    "\n",
    "if show_max is not None and show_max > 0 and show_max < len(tracker.input):\n",
    "    print(\"Displaying the first {} inputs and outputs.\".format(show_max))\n",
    "    input_frames  = [pd.DataFrame(data=tracker.input[i], columns=input_cols) for i in range(show_max)]\n",
    "    output_frames = [pd.DataFrame(data=tracker.output[i].astype(int), columns=output_cols) for i in range(show_max)]\n",
    "else:\n",
    "    print(\"Displaying all of input and output.\")\n",
    "    input_frames  = [pd.DataFrame(data=matrix, columns=input_cols)  for matrix in tracker.input]\n",
    "    output_frames = [pd.DataFrame(data=matrix.astype(int), columns=output_cols) for matrix in tracker.output]\n",
    "    \n",
    "df_list  = []\n",
    "for i in range(len(input_frames)):    \n",
    "    df_list.append(input_frames[i])\n",
    "    df_list.append(output_frames[i])\n",
    "\n",
    "print(\"Input shape:  {}\".format(tracker.input.shape))\n",
    "print(\"Output shape: {}\".format(tracker.output.shape))\n",
    "multi_column_df_display(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to load a model into our tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T17:14:46.531486Z",
     "start_time": "2017-06-27T17:14:46.519976Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T17:14:49.187799Z",
     "start_time": "2017-06-27T17:14:46.834339Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = tracker.input[0].shape # Shape of an event.\n",
    "output_shape = len(tracker.output[0][0]) # Number of tracks per event\n",
    "\n",
    "batch_size = 32\n",
    "epochs     = 256\n",
    "valsplit   = 0.25\n",
    "opt        = 'rmsprop' # optimizer\n",
    "tracker.model = Sequential()\n",
    "tracker.model.add(LSTM(32, return_sequences=True, input_shape=input_shape, dropout=.2, recurrent_dropout=.2))\n",
    "tracker.model.add(Dense(output_shape, activation='softmax'))\n",
    "\n",
    "tracker.model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "#tracker.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T17:20:18.718645Z",
     "start_time": "2017-06-27T17:14:49.773940Z"
    }
   },
   "outputs": [],
   "source": [
    "modelpath = 'simple.h5'\n",
    "hist = tracker.model.fit(tracker.input, tracker.output, epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=0, validation_split=valsplit,\n",
    "                         callbacks=[keras.callbacks.ModelCheckpoint(filepath=modelpath, verbose=0)])\n",
    "print(\"Ding! All done.\")\n",
    "winsound.Beep(1000, 1000)\n",
    "winsound.Beep(1800, 1000)\n",
    "winsound.Beep(2200, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to graph the history of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T17:20:22.844665Z",
     "start_time": "2017-06-27T17:20:18.726146Z"
    }
   },
   "outputs": [],
   "source": [
    "score, acc = tracker.model.evaluate(tracker.input, tracker.output, batch_size=batch_size)\n",
    "print(\"\\nTest Score:    {}\".format(score))\n",
    "print(\"Test Accuracy: {}\".format(acc))\n",
    "show_losses([(\"Categorical Cross Entropy\", hist)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T17:22:07.408220Z",
     "start_time": "2017-06-27T17:22:06.586862Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "predictions = tracker.model.predict(tracker.input[:len(input_frames)], batch_size=batch_size)\n",
    "\n",
    "for i, outMax in enumerate(predictions):\n",
    "    discreteOut = np.apply_along_axis(row_max, axis=1, arr=outMax)\n",
    "    acc = 0\n",
    "    showEvent = False\n",
    "    for j, x in enumerate(discreteOut):\n",
    "        if (any(np.equal(x, tracker.output[i][j].astype(int))==False)):\n",
    "            print(\"The event where the wrong hit took place is:\", i)\n",
    "            print(\"The wrong hit is in row:\", j)\n",
    "            showEvent = True\n",
    "    if showEvent==True:\n",
    "        df = [pd.DataFrame(data=discreteOut), pd.DataFrame(data=tracker.output[i]), pd.DataFrame(input_frames[i])]\n",
    "        multi_column_df_display(df)\n",
    "        acc = acc + np.count_nonzero(np.equal(x, tracker.output[i][j].astype(int)))\n",
    "    percentAcc = acc/(tracker.output[i].size)\n",
    "    print(\"Accuracy: \", percentAcc)\n",
    "    \n",
    "df = []\n",
    "for i in range(len(input_frames)):\n",
    "    df.append(input_frames[i])\n",
    "    df.append(pd.DataFrame(data=predictions[i], columns=output_cols))\n",
    "multi_column_df_display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
