{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a program that will train a model to identify and assign hits to tracks.\n",
    "Written by Daniel Zurawski & Keshav Kapoor for Fermilab Summer 2017 internship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.formatfrom mpl_toolkits.mplot3d import Axes3D as ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is borrowed from a DS&HEP tutorial.\n",
    "It is used to graph the histories after fitting a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_losses( histories ):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    #plt.ylim(bottom=0)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Error by Epoch')\n",
    "    colors=[]\n",
    "    do_acc=False\n",
    "    for label,loss in histories:\n",
    "        color = tuple([0.1, 0.1, 0.1])\n",
    "        colors.append(color)\n",
    "        l = label\n",
    "        vl= label+\" validation\"\n",
    "        if 'acc' in loss.history:\n",
    "            l+=' (acc %2.4f)'% (loss.history['acc'][-1])\n",
    "            do_acc = True\n",
    "        if 'val_acc' in loss.history:\n",
    "            vl+=' (acc %2.4f)'% (loss.history['val_acc'][-1])\n",
    "            do_acc = True\n",
    "        plt.plot(loss.history['loss'], label=l, color=color)\n",
    "        if 'val_loss' in loss.history:\n",
    "            plt.plot(loss.history['val_loss'], lw=2, ls='dashed', label=vl, color=color)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    if not do_acc: return\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    for i,(label,loss) in enumerate(histories):\n",
    "        color = tuple([0.0, 0.0, 1.0])\n",
    "        if 'acc' in loss.history:\n",
    "            plt.plot(loss.history['acc'], lw=2, label=label+\" accuracy\", color=color)\n",
    "        if 'val_acc' in loss.history:\n",
    "            plt.plot(loss.history['val_acc'], lw=2, ls='dashed', label=label+\" validation accuracy\", color=color)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a LinearTracker class.\n",
    "This class is used to load input and output from a .csv file in the correct format for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearTracker():\n",
    "    \"\"\" An object that classifies particles to tracks after an event. \"\"\"    \n",
    "    def __init__(self, dataframe, model=None):\n",
    "        \"\"\" Initialize a LinearTracker.\n",
    "            @param dataframe - pd.DataFrame - used to pick tracks from.\n",
    "                The headers should contain: (\"id\", \"z\", \"r\", \"phi\").\n",
    "            @param model - keras model - A network model that the tracker will\n",
    "                use to classify particles.\n",
    "            @return Nothing\n",
    "        \"\"\"\n",
    "        self.model     = model     # keras model to figure out tracks.\n",
    "        self.dataframe = dataframe # pandas.DataFrame for picking tracks.\n",
    "        self.input     = None      # input to train model on.\n",
    "        self.output    = None      # output to train model on.\n",
    "    # END function __init__\n",
    "    \n",
    "    def load_data(self, num_events, tracks_per_event, track_size, noise_per_event):\n",
    "        \"\"\" Load input and output data from this object's dataframe.\n",
    "            @param num_events - int - The number of events to generate.\n",
    "            @param tracks_per_event - int - The number of tracks per event.\n",
    "            @param track_size - int - The number of hits per track.\n",
    "            @param noise_per_event - int - The number of hits with no track.\n",
    "            @return Nothing\n",
    "                However, self.input and self.output become numpy arrays.\n",
    "                self.input is collection of hits of shape:\n",
    "                    (num_events, hits_per_event, 3)\n",
    "                self.output is list of probability matrices of shape:\n",
    "                    (num_events, hits_per_event, tracks_per_event)\n",
    "        \"\"\"\n",
    "        hits_per_event = (track_size * tracks_per_event) + noise_per_event\n",
    "        labels = [\"phi\", \"r\", \"z\"]\n",
    "        groups = self.dataframe[[\"id\", \"r\", \"phi\", \"z\"]].groupby(\"id\")\n",
    "        goods  = groups.filter(lambda track: len(track) == track_size)\n",
    "        bads   = groups.filter(lambda track: len(track) != track_size)\n",
    "        \n",
    "        # Populate input and output with data.\n",
    "        goods_group = [g[1] for g in list(goods.groupby(\"id\"))]\n",
    "        self.input  = np.zeros((num_events, hits_per_event, len(labels)))\n",
    "        self.output = np.zeros((num_events, hits_per_event, tracks_per_event))\n",
    "        for n in range(num_events):\n",
    "            # Retrieve a sample of tracks.\n",
    "            tracks = random.sample(goods_group, tracks_per_event)\n",
    "            \n",
    "            # Make a mapping from track ID to index within a matrix.\n",
    "            T2I = self.__get_matrix_map__(tracks) # Track to Index\n",
    "            \n",
    "            # Make some noise hits to add.\n",
    "            noise  = bads.sample(noise_per_event)\n",
    "            hits   = pd.concat(tracks + [noise]).sort_values(labels)\n",
    "            \n",
    "            self.__populate_input__(hits, labels, n)\n",
    "            self.__populate_output__(hits, T2I, n)\n",
    "    # END FUNCTION load_data\n",
    "    \n",
    "    def __data_converter__(self, data):\n",
    "        phi, r, z = data\n",
    "        return (np.cos(phi)*r, np.sin(phi)*r, z)\n",
    "    \n",
    "    def __track_colors__(self, eventID):\n",
    "        data = self.output[eventID]\n",
    "        colors = [ (0,255,255), (128,0,128), (255,255,0), (0,255,0), (0,0,255), (0,0,0)]\n",
    "        nonZeros = np.nonzero(data)\n",
    "        print('this is hitColors size')\n",
    "        print(data[0].size)\n",
    "        hitColors = [data[0].size]\n",
    "        \n",
    "        for i, hit in enumerate(nonZeros):\n",
    "            print('these are i')\n",
    "            print(i)\n",
    "            print('these are hits')\n",
    "            print(hit)\n",
    "            #hitColors[hit] = colors[i]\n",
    "        \n",
    "        return hitColors\n",
    "    \n",
    "    def create_plot(self, eventID):\n",
    "        hits = np.array([self.__data_converter__(x) for x in self.input[eventID]])\n",
    "        frame = pd.DataFrame(data=hits)\n",
    "        print()\n",
    "        #colors = self.__track_colors__(eventID)\n",
    "        fig = plt.figure()\n",
    "        plot = fig.add_subplot(111, projection='3d')\n",
    "        threeDPlot = plot.scatter(xs=frame.iloc[:,0], ys=frame.iloc[:,1], zs=frame.iloc[:,2], zdir='z', s=frame.iloc[:,0].size, c=(0, 0, 0), depthshade=True)\n",
    "        plt.show(threeDPlot)\n",
    "        \n",
    "        plt.scatter(frame.iloc[:,0], frame.iloc[:,1])\n",
    "        plt.show()\n",
    "    \n",
    "    def __populate_input__(self, hits, labels, event_index):\n",
    "        self.input[event_index, :] = hits[labels].values\n",
    "    # END FUNCTION __populate_input__\n",
    "    \n",
    "    def __populate_output__(self, hits, mapping, event_index):\n",
    "        for t, track_ID in enumerate(hits[\"id\"]):\n",
    "            index = mapping.get(track_ID)\n",
    "            if index is not None:\n",
    "                self.output[event_index, t, index] = 1\n",
    "    # END FUNCTION __populate_output__\n",
    "        \n",
    "    def __get_matrix_map__(self, tracks):\n",
    "        L = pd.concat([T.sort_values([\"r\"]).head(1) for T in tracks])\n",
    "        L.sort_values([\"phi\", \"z\"], inplace=True)\n",
    "        return dict((hit, idx) for idx, hit in enumerate(L[\"id\"]))\n",
    "    # END FUNCTION __get_matrix_map__\n",
    "# END CLASS LinearTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is how to create a LinearTracker and how to load data into it. It is important to note that after construction, a LinearTracker must call its load_data() function with user specifications for how data should be loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a ValueError describing how the population is not large enough for the sample, then that means that the data\n",
    "loaded in from the .csv file does not contain enough tracks of size 'track_size'. Try to either load in a larger\n",
    "population or change the 'track_size' variable to a different positive integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename  = ('../Data Sets/linear_data_5k.csv')\n",
    "dataframe = pd.read_csv(filename)\n",
    "tracker   = LinearTracker(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into the input and output member variables of LinearTracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "tracker.load_data(num_events=512, tracks_per_event=5, track_size=4, noise_per_event=5)\n",
    "print(\"Ding! All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the input and output training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display,HTML\n",
    "\n",
    "def multi_column_df_display(list_dfs, cols=2):\n",
    "    \"\"\" Code by David Medenjak responding to StackOverflow question found here:\n",
    "        https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side\n",
    "        Displays a list of dataframes in IPython as a table with cols number of columns.\n",
    "    \"\"\"\n",
    "    html_table = \"<table style='width:100%; border:0px'>{content}</table>\"\n",
    "    html_row = \"<tr style='border:0px'>{content}</tr>\"\n",
    "    html_cell = \"<td style='width:{width}%;vertical-align:top;border:0px'>{{content}}</td>\"\n",
    "    html_cell = html_cell.format(width=100/cols)\n",
    "\n",
    "    cells = [ html_cell.format(content=df.to_html()) for df in list_dfs ]\n",
    "    cells += (cols - (len(list_dfs)%cols)) * [html_cell.format(content=\"\")] # pad\n",
    "    rows = [ html_row.format(content=\"\".join(cells[i:i+cols])) for i in range(0,len(cells),cols)]\n",
    "    display(HTML(html_table.format(content=\"\".join(rows))))\n",
    "# END FUNCTION multi_column_df_display\n",
    "\n",
    "input_cols  = [\"phi\", \"r\", \"z\"]\n",
    "output_cols = [\"T{}\".format(i) for i in range(tracker.output.shape[2])]\n",
    "show_max    = 2\n",
    "\n",
    "if show_max is not None and show_max > 0 and show_max < len(tracker.input):\n",
    "    print(\"Displaying the first {} inputs and outputs.\".format(show_max))\n",
    "    input_frames  = [pd.DataFrame(data=tracker.input[i], columns=input_cols) for i in range(show_max)]\n",
    "    output_frames = [pd.DataFrame(data=tracker.output[i].astype(int), columns=output_cols) for i in range(show_max)]\n",
    "else:\n",
    "    print(\"Displaying all of input and output.\")\n",
    "    input_frames  = [pd.DataFrame(data=matrix, columns=input_cols)  for matrix in tracker.input]\n",
    "    output_frames = [pd.DataFrame(data=matrix.astype(int), columns=output_cols) for matrix in tracker.output]\n",
    "    \n",
    "df_list  = []\n",
    "for i in range(len(input_frames)):    \n",
    "    df_list.append(input_frames[i])\n",
    "    df_list.append(output_frames[i])\n",
    "\n",
    "print(\"Input shape:  {}\".format(tracker.input.shape))\n",
    "print(\"Output shape: {}\".format(tracker.output.shape))\n",
    "multi_column_df_display(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to load a model into our tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, LSTM, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = tracker.input[0].shape # Shape of an event.\n",
    "output_shape = len(tracker.output[0][0]) # Number of tracks per event\n",
    "print(\"Input Shape:  {}\".format(input_shape))\n",
    "print(\"Output Shape: {}\".format(output_shape))\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs     = 512\n",
    "opt        = 'rmsprop' # optimizer\n",
    "valsplit   = 0.25\n",
    "tracker.model = Sequential()\n",
    "tracker.model.add(LSTM(64, return_sequences=True, input_shape=input_shape, dropout=.2, recurrent_dropout=0.2))\n",
    "tracker.model.add(LSTM(64, return_sequences=True, dropout=.2, recurrent_dropout=.2))\n",
    "tracker.model.add(LSTM(64, return_sequences=True, dropout=.2, recurrent_dropout=.2))\n",
    "tracker.model.add(Dense(output_shape, activation='softmax'))\n",
    "\n",
    "tracker.model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "#tracker.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = tracker.model.fit(tracker.input, tracker.output, epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=0, validation_split=valsplit,\n",
    "                         callbacks=[keras.callbacks.ModelCheckpoint(filepath='simple.h5', verbose=0)])\n",
    "print(\"Ding! All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to graph the history of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = tracker.model.predict(tracker.input[:len(input_frames)], batch_size=batch_size)\n",
    "df = []\n",
    "for i in range(len(input_frames)):\n",
    "    df.append(input_frames[i])\n",
    "    df.append(pd.DataFrame(data=predictions[i], columns=output_cols))\n",
    "multi_column_df_display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
