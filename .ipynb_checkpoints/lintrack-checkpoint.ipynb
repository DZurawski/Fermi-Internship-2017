{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a program that will train a model to identify and assign hits to tracks.\n",
    "Written by Daniel Zurawski & Keshav Kapoor for Fermilab Summer 2017 internship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a LinearTracker class.\n",
    "This class is used to load input and output from a .csv file in the correct format for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearTracker():\n",
    "    \"\"\" An object that classifies particles to tracks after an event. \"\"\"    \n",
    "    def __init__(self, dataframe, model=None):\n",
    "        \"\"\" Initialize a LinearTracker.\n",
    "            @param dataframe - pd.DataFrame - used to pick tracks from.\n",
    "                The headers should contain: (\"id\", \"z\", \"r\", \"phi\").\n",
    "            @param model - keras model - A network model that the tracker will\n",
    "                use to classify particles.\n",
    "            @return Nothing\n",
    "        \"\"\"\n",
    "        self.model     = model     # keras model to figure out tracks.\n",
    "        self.dataframe = dataframe # pandas.DataFrame for picking tracks.\n",
    "        self.input     = None      # input to train model on.\n",
    "        self.output    = None      # output to train model on.\n",
    "    # END function __init__\n",
    "    \n",
    "    def load_data(self, num_events,\n",
    "                  tracks_per_event, track_size, noise_per_event):\n",
    "        \"\"\" Load input and output data from this object's dataframe.\n",
    "            @param num_events - int - The number of events to generate.\n",
    "            @param tracks_per_event - int - The number of tracks per event.\n",
    "            @param track_size - int - The number of hits per track.\n",
    "            @param noise_per_event - int - The number of hits with no track.\n",
    "            @return Nothing\n",
    "                However, self.input and self.output become numpy arrays.\n",
    "                self.input is collection of hits of shape:\n",
    "                    (num_events, hits_per_event, 3)\n",
    "                self.output is list of probability matrices of shape:\n",
    "                    (num_events, hits_per_event, tracks_per_event)\n",
    "        \"\"\"\n",
    "        hits_per_event = (track_size * tracks_per_event) + noise_per_event\n",
    "        data   = self.dataframe[[\"id\", \"r\", \"phi\", \"z\"]].drop_duplicates()\n",
    "        groups = data.groupby(\"id\")\n",
    "        valids = groups.filter(lambda track: len(track) == track_size)\n",
    "        bads   = groups.filter(lambda track: len(track) != track_size)\n",
    "        labels = [\"phi\", \"r\", \"z\"]\n",
    "        \n",
    "        # Populate input and output with data.\n",
    "        self.input  = np.zeros((num_events, hits_per_event, len(labels)))\n",
    "        self.output = np.zeros((num_events, hits_per_event, tracks_per_event))\n",
    "        for n in range(num_events):\n",
    "            # Retrieve the hits within this event.\n",
    "            sample = random.sample(list(valids.groupby(\"id\")), tracks_per_event)\n",
    "            tracks = [track[1] for track in sample] # Make it not a tuple.\n",
    "            noise  = bads.sample(noise_per_event)\n",
    "            hits   = pd.concat(tracks + [noise])\n",
    "            hits.sort_values(labels, inplace=True)\n",
    "            \n",
    "            # Populate this event's inputs.\n",
    "            self.input[n, :] = hits[labels].values\n",
    "            \n",
    "            # Define a mapping from track ID to probability matrix column.\n",
    "            T2I = dict()\n",
    "            for t, track_ID in enumerate([s[0] for s in sample]):\n",
    "                T2I[track_ID] = t\n",
    "            \n",
    "            # Populate this event's outputs.\n",
    "            for t, track_ID in enumerate(hits[\"id\"]):\n",
    "                index = T2I.get(track_ID)\n",
    "                if index is not None:\n",
    "                    self.output[n, t, index] = 1\n",
    "    # END FUNCTION load_data\n",
    "# END CLASS LinearTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is how to create a LinearTracker and how to load data into it. It is important to note that after construction, a LinearTracker must call its load_data() function with user specifications for how data should be loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a ValueError describing how the population is not large enough for the sample, then that means that the data\n",
    "loaded in from the .csv file does not contain enough tracks of size 'track_size'. Try to either load in a larger\n",
    "population or change the 'track_size' variable to a different positive integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "filename  = ('file_o_stuff3.csv')\n",
    "dataframe = pd.read_csv(filename)\n",
    "tracker   = LinearTracker(dataframe)\n",
    "tracker.load_data(num_events=10, tracks_per_event=3, track_size=4, noise_per_event=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the input training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracker.input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the output training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to load a model into our tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = tracker.input[0].shape\n",
    "\n",
    "tracker.model = Sequential()\n",
    "tracker.model.add(Dense(32, input_shape=input_shape, activation='relu'))\n",
    "tracker.model.add(Dense(output_shape, kernel_initializer='uniform'))\n",
    "tracker.model.add(Activation('softmax'))\n",
    "tracker.model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "tracker.model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
